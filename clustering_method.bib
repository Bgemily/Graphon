Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Peng2005,
abstract = {One of the fundamental clustering problems is to assign n points into k clusters based on the minimal sum-of-squares(MSSC), which is known to be NP-hard. In this paper, by using matrix arguments, we first model MSSC as a so-called 0-1 semidefinite programming (SDP). We show that our 0-1 SDP model provides an unified framework for several clustering approaches such as normalized k-cut and spectral clustering. Moreover, the 0-1 SDP model allows us to solve the underlying problem approximately via the relaxed linear and semidefinite programming. Secondly, we consider the issue of how to extract a feasible solution of the original 0-1 SDP model from the approximate solution of the relaxed SDP problem. By using principal component analysis, we develop a rounding procedure to construct a feasible partitioning from a solution of the relaxed problem. In our rounding procedure, we need to solve a K-means clustering problem in k−1 , which can be solved in O(n k 2 −2k+2) time. In case of bi-clustering, the running time of our rounding procedure can be reduced to O(n log n). We show that our algorithm can provide a 2-approximate solution to the original problem. Promising numerical results for bi-clustering based on our new method are reported.},
author = {Peng, Jiming and Wei, Yu},
file = {:Users/bgemily/Library/Application Support/Mendeley Desktop/Downloaded/Peng, Wei - 2005 - Approximating K-means-type clustering via semidefinite programming.pdf:pdf},
keywords = {0-1 SDP,Approximation,K-means clustering,Principal component analysis,Semi-definite programming},
title = {{Approximating K-means-type clustering via semidefinite programming}},
year = {2005}
}
@article{Li2017,
abstract = {Given a set of data, one central goal is to group them into clusters based on some notion of similarity between the individual objects. One of the most popular and widely-used approaches is k-means despite the computational hardness to find its global minimum. We study and compare the properties of different convex relaxations by relating them to corresponding proximity conditions, an idea originally introduced by Kumar and Kannan. Using conic duality theory, we present an improved proximity condition under which the Peng-Wei relaxation of k-means recovers the underlying clusters exactly. Our proximity condition improves upon Kumar and Kannan, and is comparable to that of Awashti and Sheffet where proximity conditions are established for projective k-means. In addition, we provide a necessary proximity condition for the exactness of the Peng-Wei relaxation. For the special case of equal cluster sizes, we establish a different and completely localized proximity condition under which the Amini-Levina relaxation yields exact clustering, thereby having addressed an open problem by Awasthi and Sheffet in the balanced case. Our framework is not only deterministic and model-free but also comes with a clear geometric meaning which allows for further analysis and generalization. Moreover, it can be conveniently applied to analyzing various data generative models such as the stochastic ball models and Gaussian mixture models. With this method, we improve the current minimum separation bound for the stochastic ball models and achieve the state-of-the-art results of learning Gaussian mixture models.},
archivePrefix = {arXiv},
arxivId = {1710.06008},
author = {Li, Xiaodong and Li, Yang and Ling, Shuyang and Strohmer, Thomas and Wei, Ke},
eprint = {1710.06008},
file = {:Users/bgemily/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - When Do Birds of a Feather Flock Together k-Means, Proximity, and Conic Programming(2).pdf:pdf},
month = {oct},
title = {{When Do Birds of a Feather Flock Together? k-Means, Proximity, and Conic Programming}},
url = {http://arxiv.org/abs/1710.06008},
year = {2017}
}
@techreport{Neuristique,
abstract = {This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm.},
author = {Neuristique, Leon Bottou and Bengio, Yoshua},
file = {:Users/bgemily/Library/Application Support/Mendeley Desktop/Downloaded/Neuristique, Bengio - Unknown - Convergence Properties of the K-Means Algorithms.pdf:pdf},
title = {{Convergence Properties of the K-Means Algorithms}}
}
@techreport{Ling2019,
abstract = {Spectral clustering has become one of the most widely used clustering techniques when the structure of the individual clusters is non-convex or highly anisotropic. Yet, despite its immense popularity, there exists fairly little theory about performance guarantees for spectral clustering. This issue is partly due to the fact that spectral clustering typically involves two steps which complicated its theoretical analysis: first, the eigenvectors of the associated graph Laplacian are used to embed the dataset, and second, k-means clustering algorithm is applied to the embedded dataset to get the labels. This paper is devoted to the theoretical foundations of spectral clustering and graph cuts. We consider a convex relaxation of graph cuts, namely ratio cuts and normalized cuts, that makes the usual two-step approach of spectral clustering obsolete and at the same time gives rise to a rigorous theoretical analysis of graph cuts and spectral clustering. We derive deterministic bounds for successful spectral clustering via a spectral proximity condition that naturally depends on the algebraic connectivity of each cluster and the inter-cluster connectivity. Moreover, we demonstrate by means of some popular examples that our bounds can achieve near-optimality. Our findings are also fundamental to the theoretical understanding of kernel k-means. Numerical simulations confirm and complement our analysis.},
archivePrefix = {arXiv},
arxivId = {1806.11429v3},
author = {Ling, Shuyang and Strohmer, Thomas},
eprint = {1806.11429v3},
file = {:Users/bgemily/Library/Application Support/Mendeley Desktop/Downloaded/Ling, Strohmer - 2019 - Certifying Global Optimality of Graph Cuts via Semidefinite Relaxation A Performance Guarantee for Spectral Clus.pdf:pdf},
title = {{Certifying Global Optimality of Graph Cuts via Semidefinite Relaxation: A Performance Guarantee for Spectral Clustering}},
year = {2019}
}
@article{Li2018,
abstract = {This paper surveys recent theoretical advances in convex optimization approaches for community detection. We introduce some important theoretical techniques and results for establishing the consistency of convex community detection under various statistical models. In particular, we discuss the basic techniques based on the primal and dual analysis. We also present results that demonstrate several distinctive advantages of convex community detection, including robustness against outlier nodes, consistency under weak assortativity, and adaptivity to heterogeneous degrees. This survey is not intended to be a complete overview of the vast literature on this fast-growing topic. Instead, we aim to provide a big picture of the remarkable recent development in this area and to make the survey accessible to a broad audience. We hope that this expository article can serve as an introductory guide for readers who are interested in using, designing, and analyzing convex relaxation methods in network analysis.},
archivePrefix = {arXiv},
arxivId = {1810.00315},
author = {Li, Xiaodong and Chen, Yudong and Xu, Jiaming},
eprint = {1810.00315},
file = {:Users/bgemily/Library/Application Support/Mendeley Desktop/Downloaded/Li, Chen, Xu - 2018 - Convex Relaxation Methods for Community Detection.pdf:pdf},
month = {sep},
title = {{Convex Relaxation Methods for Community Detection}},
url = {http://arxiv.org/abs/1810.00315},
year = {2018}
}

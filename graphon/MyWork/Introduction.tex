%!TEX root = Main.tex


\section{Introduction}
~ 

\subsection*{Topic introduction and motivation}
\begin{itemize}
	\item Networks arise in many disciplines. [Cite some applied papers.]
	\item One major topic in network analysis is the study of node classification. [Introduce its application in neuroscience, social science, etc.]
	\item In this paper, we focus on the problem of clustering cell types of neurons in a developing network. 
	We observe the time of the appearance of edges in the network, and the edges do not disappear once developed.
	Neurons of the same type have similar active time and connecting patterns.
	\item While existing dynamic stochastic block models show promising performance on some real datasets, they do not fit our dataset for the following reasons.
	 (i) Most of the dynamic stochastic block models are designed for the snapshots of a network. But in our case, the network is sparse in the sense that there is at most one event (occurrence of an edge) for each pair of neurons. For this reason, one cannot model the snapshots by (dynamic) stochastic block models.
	 (ii) The assumption that all the nodes within the same cluster are statistically equivalent is violated. In our network, neurons only develop edges with their neighbor neurons. So two neurons from the same cluster may connect with two different subsets of neurons from another cluster.
	\item We address these problems by an extension of the stochastic block model.
\end{itemize}

\subsection*{Contribution}
\begin{itemize}
	\item We integrate the point processes in order to obtain the statistical equivalence of nodes from the same cluster.
	\item We derive a k-means (or least square) estimator and show its consistency (or the oracle inequality). This is similar to \citet{Pensky2019a}, except that we also incorporate time lag and spatial position which may lower the estimation precision.
	\item For inference, we (hopefully) propose a convex relaxation method that convexify over both clustering matrix and time lags. We also obtain the proximity condition for this convex relaxation method to recover the global optima. (Convex relaxation can also be adapted to solve the penalized least square problem in \citet{Pensky2019a}. )
\end{itemize}




\subsection*{Related work}
\begin{itemize}
	\item There are many dynamic extensions of stochastic block model. 
	\citet{Yang2011,Xu2014a,Matias2016,Xu2015} use the Markov chain to model the time-varying  connecting probabilities and/or the clustering matrix. 
	EM algorithm or iterative optimization algorithm is commonly used for inference.
	\citet{Pensky2019a} proposes a vectorization technique and derives a penalized least square estimator that satisfies an oracle inequality and  attains the minimax lower bound for the risk.
	\item However, these methods are based on snapshots of networks where data are aggregated on some time intervals. 
	Aggregating data will lose much information, and the results can be crucially influenced by the choice of time intervals. 
	\item \citet{Matias2018} address this problem by extending the stochastic block model to recurrent interaction events in continuous time, where the time-varying connecting probabilities are characterized by intensity functions.
	\item While this model gives reasonable interpretation on some real datasets, it does not guarantee to recover the clusters determined by the connecting pattern of each node. Moreover, the model comes without theoretical guarantees for the estimation precision. 
	\item Convex relaxation methods for community detection have been studied in \citet{Li2018,Li2017,Peng2005}.
	\item {\color{blue} Degree-corrected model?}

\end{itemize}





\subsection*{Organization}




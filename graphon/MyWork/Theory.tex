%!TEX root = Main.tex


\section{Main result}


% \begin{thm}
% \end{thm}

% \begin{proof}
% \end{proof}

	\subsection{Consistency of k-means type method}
		\citet{Pollard1981a} showed the almost sure convergence, as the sample size increases, of the set of means of the k clusters. The result can be generalized to any metric space for which all the closed balls are compact would do.

	\subsection{Convergence of Lloyd's algorithm using shape invariant model}
		\subsubsection*{Guarantees of Lloyd's algorithm}
			\citet{Lu} provides a weak initialization condition under which Lloyd's algorithm converges to the optimal label estimators of sub-Gaussian mixture model.
			Also, see the reference therein.
		\subsubsection*{Consistency of the shape invariant model}

	% or other intensity estimation method
		Consistent in terms of both number of curves and number of observed points go to infinity.

	\subsection{Proximity condition of the convex relaxation}\label{sec:proximity condition}
		See \citet{Ling2019,Li2017,Peng2005,Zhao2012} for theory and proof.
		\citet{Ling2019} proposed a proximity condition under which the convex relaxation of RatioCuts is exactly the global optimal to the original ratio cut problem. The theorem is then applied to spectral clustering {(as a special case of graph cuts)} to obtain the theoretical guarantees for spectral clustering
		\\
		\citet{Li2018} surveys recent theoretical advances in convex optimization approaches for community detection.
		\citet{Li2017} compare different convex relaxations by relating them to corresponding proximity conditions. They present an improved proximity condition under which the relaxation proposed by \citet{Peng2005} recovers the underlying clusters exactly.
		The proximity condition states that for any $a\neq b$, the following holds
		\begin{align*}
		\min _{i\in\Gamma_a}\left\langle \mathbf{x}_{i}-\frac{\mathbf{c}_{a}+\mathbf{c}_{b}}{2}, \mathbf{w}_{b, a}\right\rangle>\frac{1}{2} \sqrt{\left(\sum_{l=1}^{k}\left\|\overline{\mathbf{X}}_{l}\right\|^{2}\right)\left(\frac{1}{n_{a}}+\frac{1}{n_{b}}\right)} 
		\end{align*}
		where $\mathbf{w}_{b,a} = \frac{\mathbf{c}_a - \mathbf{c}_b}{\| \mathbf{c}_a - \mathbf{c}_b \|}$ is the unit vector pointing from $\mathbf{c}_b$ to $\mathbf{c}_a$,
		$\overline{\mathbf{X}}_l$ is the centered data matrix of the $l$-th cluster, $\|\overline{\mathbf{X}}_l\|$  is the operator norm of $\overline{\mathbf{X}}_l$, and $n_a=|\Gamma_a|,n_b=|\Gamma_b|$.
		\\
		\textcolor{red}{It seems to require the clusters are well-separated?}
		


	
		




		
	% \subsection{Consistency of the convex relaxation}
	% both the clustering matrix and the intensity functions
	% How to define the convergence of clustering and functions?

	% \subsection{Optimality of the estimator}
	% Based on Pensky's paper
%!TEX root = Main.tex


\section{Theory} \label{sec:theory}

Assume $ \mathbf{F}=\left( F_i \right)_{i=1}^n$ is from the parameter space
\begin{align*}
\mathcal F_k = XXX.
\end{align*}


\begin{thm}
For any constant $C'>0$, there is a constant $C>0$ only depending on $C'$, such that
\begin{align*}
\frac{1}{n}\sum_{i=1}^n \left\| \hat F_i - F_i \right\|^2\leq C \left( XXX \right)  ,
\end{align*}
with probability at least $1-\exp \left( -C' XXX \right)  $, uniformly over $\mathbf{F}\in \mathcal F_k$.

\end{thm}

\begin{proof}
This is a sketch of proof and is based on the proof in \cite{Gao2015a}.

We denote the true value by $\theta^*_i = F^*_{z_i^*}(\cdot - \tau_i^*)$. 
For the estimated $\hat z$, define $ \tilde \theta = \underset{\theta\in\Theta_k(\hat z)}{\arg\min} \| \theta^* - \theta\|^2 $.
By the definition of the estimator, we  have
\begin{align*}
L(\hat F, \hat Z, \hat \tau) \leq L(F^*, Z^*, \tau^*),
\end{align*}
which can be rewritten as
\begin{equation}\label{4.1}
\| \hat \theta - F^{obs}\|^2 \leq \|\theta^* - F^{obs}\|^2.
\end{equation}
The left-hand side of \eqref{4.1} can be decomposed as
\begin{equation}\label{4.2}
\| \hat \theta - \theta^*\|^2 + 2\langle \hat \theta - \theta^*, \theta^* - F^{obs} \rangle + \| \theta^* - F^{obs}\|^2.
\end{equation}
Combining \eqref{4.1} and \eqref{4.2}, we have
\begin{equation}\label{4.3}
\| \hat \theta - \theta ^* \|^2 \leq 2 \langle \hat \theta - \theta^*, F^{obs}-\theta^* \rangle.
\end{equation}
The right-hand side of \eqref{4.3} can be bounded as 
\begin{align}
\nonumber
\langle \hat \theta - \theta^*, F^{obs} - \theta^*\rangle 
&= 
\langle \hat \theta - \tilde\theta, F^{obs} - \theta^*\rangle 
+ \langle \tilde \theta - \theta^*, F^{obs} - \theta^*\rangle \\
\label{4.4}
&\leq 
\| \hat \theta - \tilde \theta \|  
\left| \left\langle \frac{\hat \theta - \tilde \theta}{\| \hat \theta - \tilde \theta \|}, F^{obs} - \theta^* \right\rangle \right|\\
\label{4.5}
& \quad + 
\left( \| \tilde \theta - \hat \theta \| +  \| \hat \theta -  \theta^* \| \right) 
\left| \left\langle \frac{\tilde \theta -  \theta^*}{\| \tilde \theta - \theta^* \|}, F^{obs} - \theta^* \right\rangle \right|.
\end{align}
Using Lemmas XXX, the following three terms:
\begin{equation}\label{4.6}
\| \hat \theta - \tilde \theta \| , 
\quad 
\left| \left\langle \frac{\hat \theta - \tilde \theta}{\| \hat \theta - \tilde \theta \|}, F^{obs} - \theta^* \right\rangle \right|,
\quad
\left| \left\langle \frac{\tilde \theta -  \theta^*}{\| \tilde \theta - \theta^* \|}, F^{obs} - \theta^* \right\rangle \right|
\end{equation}
can all be bounded by XXX with probability at least
\begin{align*}
XXX.
\end{align*}
Combining these bounds with \eqref{4.4}, \eqref{4.5}
and \eqref{4.3}, we get
\begin{align*}
\|\hat \theta - \theta^* \|^2 \leq XXX
\end{align*}
with probability at least XXX. 
\end{proof}

Now we present the lemmas, which bound the three terms in \eqref{4.6}, respectively. 
\begin{lemma}\label{lemma:1}
For any constant $C'>0$, there exists a constant $C>0$ only depending on $C'$, such that
\begin{align*}
\|\hat \theta - \tilde{\theta} \| \leq C XXX,
\end{align*}
with probability at least XXX.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:1}]
~

\textit{Step 1: Control $\mathbb{E}\|\hat\tau - \tilde\tau\|^2$.}

Fix a group $\hat\Gamma_k$, denote the group size by $J=|\Gamma_k|$.
For notation simplicity, relabel the nodes in group $\hat\Gamma_k$ with $\left\{ 1,\cdots,J \right\}$.
Let 
\begin{align*}
M(\tau_1,\cdots,\tau_{J}) 
= \frac{1}{J}\sum_{j=1}^J 
\sum_{|k|\leq k_0} \left| 
 c_{j,k} e^{\complexunit{}2\pi k \tau_j} 
 - \frac{1}{J}\sum_{j'=1}^J c_{j',k} e^{\complexunit{}2\pi k \tau_{j'}} 
 \right|^2,
\end{align*}
where $c_{j,k},k\in\mathbb{Z},$ are the Fourier coefficients of the distribution function $F^*_{z^*_j}(\cdot-\tau^*_j)$.
~
Let 
\begin{align*}
\hat M(\tau_1,\cdots,\tau_{J}) 
= \frac{1}{J}\sum_{j=1}^J 
\sum_{|k|\leq k_0} \left| 
 \hat c_{j,k} e^{\complexunit{}2\pi k \tau_j} 
 - \frac{1}{J}\sum_{j'=1}^J \hat c_{j',k} e^{\complexunit{}2\pi k \tau_{j'}} 
 \right|^2,
\end{align*}
where $\hat c_{j,k},k\in\mathbb{Z},$ are the Fourier coefficients of the empirical distribution function $F^{obs}_j$.
Then $\hat\tau$ is the minimizer of $\hat M$, and $\tilde\tau$ is the minimizer of $M$.
\\
By Proposition 3.1 in \citet{Bigot2013}, under proper assumptions of $F$ and distribution of $\tau$, 
\begin{align*}
\frac{1}{J}\|\hat\tau-\tilde\tau\|^2\leq C^{-1}\cdot(M(\hat\tau_1,\cdots,\hat\tau_J)-M(\tilde\tau_1,\cdots,\tilde\tau_J)).
\end{align*}
Note that $M(\hat\tau)-M(\tilde{\tau})=M(\hat\tau)-\hat M(\hat{\tau})+\hat M(\hat{\tau})-M(\tilde{\tau})\leq 2~\sup_{\tau}|M(\tau)-\hat M(\tau)| $, so it suffices to control
$\mathbb{E}\sup_{\tau}|M(\tau)-\hat M(\tau)|$.

$\mathbb{E}\sup_{\tau}|M(\tau)-\hat M(\tau)|$ is controlled by $F_j^{obs}-F^*_j$ or $\hat c_{j,k}-c_{j,k}$?

\textit{Step 2: Bound $\|\hat \theta - \tilde \theta\|$.}




\end{proof}


\begin{lemma}
For any constant $C'>0$, there exists a constant $C>0$ only depending on $C'$, such that
\begin{align*}
\left| \left\langle \frac{\tilde \theta -  \theta^*}{\| \tilde \theta - \theta^* \|}, F^{obs} - \theta^* \right\rangle \right|
 \leq C XXX,
\end{align*}
with probability at least XXX.
\end{lemma}

\begin{proof}
Note that
\begin{align*}
\tilde \theta_i - \theta^*_i 
= \tilde F_{\hat z_i}(\cdot - \tilde \tau_i) - F^*_{z^*_i}(\cdot-\tau^*_i)
\end{align*}
is a function of the partition $\mathbf{\hat z}$, then we have
\begin{align*}
\left| \sum_i 
\left\langle 
\frac{\tilde \theta_i-\theta^*_i}{\sqrt{\sum_i \| \tilde \theta_i - \theta^*_i \|^2}} 
, F_i^{obs}-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle   
\right|
\leq \max_{\mathbf{z}\in \mathcal Z_{n,k}}
\left | 
 \sum_{i} \left\langle \gamma_i(\mathbf{z})
 , F_i^{obs}-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle   \right|
\end{align*}
where 
\begin{align*}
\gamma_i(\mathbf{z})\propto 
\tilde F_{ z_i}(\cdot - \tilde \tau_i) - F^*_{z^*_i}(\cdot-\tau^*_i)
\end{align*}
satisfies $\sum_i \|\gamma_i(\mathbf{z})\|^2=1$.
By [some inequality similar to Hoeffding's inequality] and union bound, we have
\begin{align*}
&\mathbb{P}\left( 
\max_{\mathbf{z}\in \mathcal Z_{n,k}} 
\left| \sum_{i} 
\left\langle \gamma_i(\mathbf{z}), F^{obs}_i-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle 
\right| 
>t \right) \\
\leq& \sum_{z\in \mathcal Z_{n,k}}
\mathbb{P} 
\left( \left| \sum_{i} 
\left\langle \gamma_i(\mathbf{z}), F^{obs}_i-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle 
\right| 
>t \right) \\
\leq & |\mathcal Z_{n,k}| \exp \left( -C_1t^2 \right) ,
\end{align*}
for some universal constant $C_1>0$.
Choosing $t\propto \sqrt{n\log k} $, the proof is complete.




\end{proof}


\begin{lemma}
For any constant $C'>0$, there exists a constant $C>0$ only depending on $C'$, such that
\begin{align*}
\left| \left\langle \frac{\hat \theta - \tilde \theta}{\| \hat \theta - \tilde \theta \|}, F^{obs} - \theta^* \right\rangle \right|
 \leq C XXX,
\end{align*}
with probability at least XXX.
\end{lemma}

\begin{proof}
Note that
\begin{align*}
\hat \theta_i - \tilde \theta_i 
= \hat F_{\hat z_i}(\cdot - \hat \tau_i) - \tilde F_{\hat z_i}(\cdot-\tilde\tau_i)
\end{align*}
is a function of both the partition $\mathbf{\hat z}$ and the observations $F^{obs}$.
For each $\mathbf{z}\in \mathcal Z_{n,k}$, define the set $\mathcal B_\mathbf{z}$ by
\begin{align*}
XXX
\end{align*}
Thus, we have the bound
\begin{align*}
 \left| \sum_i 
\left\langle 
\frac{\tilde \theta_i-\theta^*_i}{\sqrt{\sum_i \| \tilde \theta_i - \theta^*_i \|^2}} 
, F_i^{obs}-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle   
\right| 
\leq 
\max_{\mathbf{z}\in \mathcal Z_{n,k}}
\sup_{c\in \mathcal B_{\mathbf{z}}}
\left | 
 \sum_{i} \left\langle c_i
 , F_i^{obs}-F^*_{z^*_i}(\cdot-\tau^*_i) \right\rangle   \right|
 \end{align*}
If set $\mathcal B_{\mathbf{z}}$ is not too large, applying union bound (and Hoeffding-like inequality) completes the proof.

\end{proof}


	% \subsection{Minimax optimality of the estimator}
	% 	For the case in \eqref{eq:kmeans_lambda}, denote $\mathbf{\theta}=\left( \{\Gamma_l\}_{l=1}^k, \left\{ \tau_i \right\}_{i=1}^n, \left\{ \lambda_l \right\}_{l=1}^k \right)$, show the convergence rate of
	% 		\begin{align*}
	% 		\hat\theta_n := 
	% 		\underset{\{\Gamma_l\}_{l=1}^k, \left\{ \tau_i \right\}_{i=1}^n, \left\{ \lambda_l \right\}_{l=1}^k}{\arg\min}
	% 		\frac{1}{n} \sum_{l=1}^k \sum_{i\in\Gamma_l} \|\hat\lambda_{N_i}(\cdot+\tau_i)- \lambda_l(\cdot)\|_2^2 
	% 		\end{align*}
	% 	to 
	% 		\begin{align*}
	% 			\theta_n := 
	% 			\underset{\{\Gamma_l\}_{l=1}^k, \left\{ \tau_i \right\}_{i=1}^n, \left\{ \lambda_l \right\}_{l=1}^k}{\arg\min}
	% 			\frac{1}{n} \sum_{l=1}^k \sum_{i\in\Gamma_l} \|\lambda_{N_i}(\cdot+\tau_i)- \lambda_l(\cdot)\|_2^2.
	% 		\end{align*}
	% 	Papers about shape invariant model might be useful. 
		
		
		
		
		
		

		% Likelihood case

		% \citet{Pollard1981a} showed the almost sure convergence, as the sample size increases, of the set of means of the k clusters. The result can be generalized to any metric space for which all the closed balls are compact would do.


	% \subsection{Convergence of Lloyd's algorithm using shape invariant model}
	% 	\subsubsection*{Guarantees of Lloyd's algorithm}
	% 		\citet{Lu} provides a weak initialization condition under which Lloyd's algorithm converges to the optimal label estimators of sub-Gaussian mixture model.
	% 		Also, see the reference therein.

	% or other intensity estimation method
		% Consistent in terms of both number of curves and number of observed points go to infinity.

	% \subsection{Proximity condition of the convex relaxation}\label{sec:proximity condition}
	% 	See \citet{Ling2019,Li2017,Peng2005,Zhao2012} for theory and proof.
	% 	\citet{Ling2019} proposed a proximity condition under which the convex relaxation of RatioCuts is exactly the global optimal to the original ratio cut problem. The theorem is then applied to spectral clustering {(as a special case of graph cuts)} to obtain the theoretical guarantees for spectral clustering
	% 	\\
	% 	\citet{Li2018} surveys recent theoretical advances in convex optimization approaches for community detection.
	% 	\citet{Li2017} compare different convex relaxations by relating them to corresponding proximity conditions. They present an improved proximity condition under which the relaxation proposed by \citet{Peng2005} recovers the underlying clusters exactly.
	% 	The proximity condition states that for any $a\neq b$, the following holds
	% 	\begin{align*}
	% 	\min _{i\in\Gamma_a}\left\langle \mathbf{x}_{i}-\frac{\mathbf{c}_{a}+\mathbf{c}_{b}}{2}, \mathbf{w}_{b, a}\right\rangle>\frac{1}{2} \sqrt{\left(\sum_{l=1}^{k}\left\|\overline{\mathbf{X}}_{l}\right\|^{2}\right)\left(\frac{1}{n_{a}}+\frac{1}{n_{b}}\right)} 
	% 	\end{align*}
	% 	where $\mathbf{w}_{b,a} = \frac{\mathbf{c}_a - \mathbf{c}_b}{\| \mathbf{c}_a - \mathbf{c}_b \|}$ is the unit vector pointing from $\mathbf{c}_b$ to $\mathbf{c}_a$,
	% 	$\overline{\mathbf{X}}_l$ is the centered data matrix of the $l$-th cluster, $\|\overline{\mathbf{X}}_l\|$  is the operator norm of $\overline{\mathbf{X}}_l$, and $n_a=|\Gamma_a|,n_b=|\Gamma_b|$.
	% 	\\
	% 	% {It also requires that the clusters are well-separated.}
	% 	\\
	% 	{\color{blue} Translate into point processes if \eqref{eq:unconvexified k-means} can be convexified.}
		


	
		




		
	% \subsection{Consistency of the convex relaxation}
	% both the clustering matrix and the intensity functions
	% How to define the convergence of clustering and functions?

	% \subsection{Optimality of the estimator}
	% Based on Pensky's paper
%  
% This is the root file
% Proofs are in the file  DSBM_AOS_Proofs_March25_2018.tex
% Supplemental file is DSBM_AOS_Suppl_March25_2018.tex


\documentclass[aos,preprint]{imsart}

%\RequirePackage[OT1]{fontenc}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\usepackage{graphicx}
%\usepackage{textcomp}
\usepackage{amsthm,amsmath,natbib}
\usepackage{amsfonts}
%\usepackage{epsfig}
\usepackage{color}
 



 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\startlocaldefs

\input{Definitions.tex}


\endlocaldefs

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{document}

\begin{frontmatter}


\title
{\bf      DYNAMIC NETWORK MODELS AND GRAPHON ESTIMATION}

\runtitle{DYNAMIC NETWORK MODELS AND GRAPHON ESTIMATION} 
\author{\fnms{Marianna Pensky}\thanksref{t1}\ead[label=e1]{Marianna.Pensky@ucf.edu}}
\runauthor{M. Pensky}
\thankstext{t1}{Supported in part by National Science Foundation (NSF),
grants  DMS-1407475 and DMS-1712977}



\affiliation{University of Central Florida}

\address
{Marianna Pensky\\
Department of Mathematics \\
University of Central Florida \\
Orlando FL 32816-1354, USA \\
\printead{e1}}


 

\begin{abstract}
In the present paper we consider a dynamic stochastic network model.
The objective is estimation of the tensor of connection probabilities $\bLam$
when it is generated by a Dynamic Stochastic Block Model (DSBM) or a dynamic graphon.
In particular, in the context of the DSBM, we derive a penalized least squares   estimator  $\hbLam$  of $\bLam$ 
and show that $\hbLam$ satisfies an oracle inequality and also attains minimax lower bounds for the risk.  
We extend  those results to estimation of $\bLam$ when it is generated by a dynamic graphon function.
The estimators constructed in the paper are adaptive to the unknown number of blocks in the context of the DSBM or
to the smoothness of the graphon function.  
The technique relies on the vectorization of the model and leads to 
much simpler mathematical arguments  than the ones used previously in the stationary set up.
In addition, all  results in the paper are non-asymptotic and allow a variety of  extensions.  
\end{abstract}

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\begin{keyword}[class=MSC]
\kwd[Primary ]{60G05}
\kwd[; secondary ]{05C80, 62F35}
\end{keyword}

\begin{keyword}
\kwd{dynamic network}
\kwd{graphon}
\kwd{stochastic block model}
\kwd{nonparametric regression}
\kwd{minimax rate}
\end{keyword}

\end{frontmatter}


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

 

\section{Introduction}
\label{sec:introduction}
% \setcounter{equation}{0}
 
Networks arise in many areas of research such as sociology, biology, genetics, ecology, information technology
to list a few. An overview of statistical modeling of random graphs can be found in, e.g.,   Kolaczyk (2009) and 
Goldenberg \etal (2011). While  static network models are relatively well understood, the literature on the dynamic 
network models is fairly recent.


In this paper, we consider a dynamic network defined as an undirected graph with $n$ nodes
with connection probabilities changing in time.
Assume that we observe the values of a tensor $\bB_{i,j,l} \in \{ 0,1\}$
at times  $t_l$ where $0 < t_1 < \cdots < t_L =T$. 
For simplicity, we assume that time instants are equispaced and the time interval is scaled to one, i.e. $t_l = l/L$.
Here $\bB_{i,j,l}=1$ if a connection between nodes $i$ and $j$ is observed at time $t_l$ and 
$\bB_{i,j,l}=0$  otherwise. 
We set $\bB_{i,i,l}=0$ and  $\bB_{i,j,l} = \bB_{j,i,l}$ for any $i,j = 1, \cdots n$ and $l=1, \cdots, L$,
and assume that $\bB_{i,j,l}$ are independent Bernoulli random variables with $\bLam_{i,j,l} = \PP(\bB_{i,j,l}=1)$
and $\bLam_{i,i,l}=0$.
% 
Below, we study two types of objects:   a Dynamic Stochastic Block Model (DSBM) and a dynamic graphon.


The DSBM can be viewed as a natural extension of the Stochastic Block Model  (SBM) which, according to
Olhede and Wolfe  (2014), provides an  universal tool for description of time-independent stochastic network data.
In a DSBM,    all $n$ nodes are grouped into $m$ classes $\Om_1, \cdots, \Om_m$,
and probability of a connection $\bLam_{i,j,l}$ is entirely  determined by the  groups to which the nodes $i$ and $j$ 
belong at the moment $t_l$. In particular,  if $i \in \Om_{k}$ and $j \in \Om_{k'}$, then 
$\bLam_{i,j,l} = \bG_{k,k',l}$. Here, $\bG$ is the {\it connectivity tensor} at time $t_l$  with $\bG_{k,k',l} = \bG_{k',k,l}$.  
Denote by $n_k^{(l)}$ the number of nodes in class $k$ at the moment $t_l$, $k=1, \ldots, m$, $l=1, \ldots, L$. 


 A dynamic graphon can be defined as follows.
Let $\bzeta = (\zeta_1, \cdots, \zeta_n)$ be a random vector sampled from a distribution $\PP_\zeta$
supported on $[0,1]^n$. Although the most common choice for $\PP_\zeta$ is the i.i.d. uniform distribution 
for each $\zeta_i$, we do not make this assumption in the present paper. 
We further assume that there exists a   function $f: [0,1]^3 \to [0,1]$ such that 
for any $t$ one has $f(x,y,t) = f(y,x,t)$ and  
% If we set 
\be \label{graphon}
\bLam_{i,j,l} = f(\zeta_i, \zeta_j, t_l), \quad i,j = 1, \cdots,n, \ l=1, \cdots, L.
\ee
Then, function $f$ summarizes behavior of the network and can be  called   {\it dynamic graphon},
similarly to the graphon  in the situation of a stationary network. This formulation allows   
to study a different set of  stochastic network models than the DSBM.


It is known that graphons play an important role in the theory of graph limits
described in Lov\'{a}sz and Szegedy (2006) and   Lov\'{a}sz (2012).
The definition of the dynamic graphon above fully agrees with their theory.
Indeed, for every   $l=1, \cdots, L$, the limit of  $\bLam_{*,*,l}$ as $n \to \infty$ 
is $f(\cdot,\cdot, t_l)$. We shall further elaborate on the notion of the dynamic graphon 
in Section~\ref{sec:dyn_graphon}.



\ignore{
Given an observed adjacency tensor $\bB$ sampled according to model \fr{graphon}, 
the graphon function $f$ is not identifiable since the topology of  a network 
is invariant with respect to any change of labeling of its nodes. Therefore, 
for any $f$ and any measure-preserving bijection $\mu: [0,1]\to [0,1]$
(with respect to Lebesgue measure), the functions $f(x,y,t)$ and $f(\mu(x), \mu(y),t)$
define the same probability distribution on random graphs. For this reason, we are 
considering equivalence classes of graphons. Note that in order for it to be possible to compare 
clustering of nodes across time instants, we introduce an assumption 
% (which, to the best of our knowledge, first appeared in Matias and Miele (2015)) 
that there are no label switching in time, that is, every  
node carries the same label at any time $t_l$, so that function $\mu$ is independent of $t$.


In addition, in our model we assume that, for every $x$ and $y$,
functions $f(x,y,\ldot)$ are smooth.   
}

 

In the last few years, dynamic network models attracted a great deal of attention
(see, e.g., Durante \etal (2015), Durante \etal   (2016), Han \etal (2015), 
Kolar \etal (2010), Leonardi \etal (2016), Matias and  Miele (2015),  Minhas \etal (2015),
Xing \etal (2010), Xu (2015), Xu and Hero III (2014) and Yang \etal  (2011) among others).
Majority of those paper describe   changes in the connection probabilities and 
group memberships via various  kinds of   Bayesian or Markov random field models 
and carry out the inference using the EM or iterative optimization algorithms.
While procedures described in those papers show good computational properties, 
they come without guarantees for the estimation precision. The only paper 
known to us that is concerned with estimation precision in the  dynamic  setting is by
Han \etal (2015) where the authors study consistency of their procedures 
when $n \to \infty$ or $L \to \infty$. 


% 
On the other hand,  recently,  several authors carried out  minimax studies
 in the context of stationary network  models. In particular, Gao \etal (2015) 
developed   upper and   minimax lower bounds for the risk of estimation of the   
matrix of connection probabilities. In a subsequent paper, Gao \etal (2016)
generalized the results to a somewhat more general problem of estimation of matrices with bi-clustering structures.
In addition, Klopp \etal (2017) extended these results to the case when the network is sparse in a sense 
that probability of connection is uniformly small and tends to zero as $n \to \infty$.
Also, Zhang and  Zhou (2016) investigated minimax rates  of community detection in the two-class 
stochastic block model. 




The present paper has several objectives. First, we describe the non-parametric DSBM
 model  that allows  for smooth evolution of the tensor $\bG$ of connection probabilities  as well
as changes in group memberships in time.  Second,  we introduce   vectorization of the model
that enables us to take advantage of   well studied methodologies in nonparametric regression estimation. 
Using these techniques, we derive penalized least squares   estimators $\hbLam$ of $\bLam$ 
and show that they satisfy oracle inequalities. These inequalities do not require any assumptions on the mechanism
that drives evolution of the group memberships of the nodes in time and can be applied 
under very mild conditions. Furthermore, we consider a particular situation where only 
at most $n_0$ nodes can change their memberships between two consecutive time points.
Under the latter assumption, we derive minimax lower bounds 
for the risk of an  estimator of $\bLam$ and confirm that the estimators constructed in the paper  
attain those lower bounds.  Moreover, we extend those results to estimation 
of the   tensor $\bLam$ when it is generated by a graphon function.
We show that, for  the graphon, the estimators are minimax optimal within a logarithmic factor of $L$.
Estimators, constructed in the paper,  do not require   knowledge of the number of classes $m$ 
 in the context of the DSBM, or a degree of smoothness of the 
graphon function $f$ if $\bLam$ is generated by a dynamic graphon. 
  


 
Note that unlike in Klopp \etal (2016) we do not consider a  network that is sparse 
in a sense that probabilities of connections between classes are  uniformly small. 
However, since our technique is based on model selection, it allows to study a network 
where some groups do not communicate with each other and obtain more accurate results.
Moreover, as we show in Section \ref{sec:sparse}, by adjusting the penalty, one can 
provide adaptation to uniform sparsity assumption if the number of nodes in each class is large enough.



 


The present paper makes several key contributions. 
%
First, to the best of our knowledge, the time-dependent networks are  usually handled via generative models that 
assume some probabilistic mechanism which governs the evolution  of the network  in time. The present paper offers 
the first fully non-parametric  model for the time-dependent networks which does not make any of such assumptions. 
It treats connection probabilities for each group as  functional data, allows group membership switching   
and enables one to exploit  stability in the group memberships over time.
%
Second, the paper provides the first minimax study of estimation of the tensor of connection probabilities in a dynamic setting.
The estimators constructed in the paper  are adaptive to the number of blocks in the context of the DSBM and 
to the smoothness of the graphon function in the case of a dynamic graphon. Moreover, the approach of the paper is  non-asymptotic, 
so it can be used irrespective of how large the number of nodes $n$, the number of groups $m$ and the number of time instants $L$ are and 
what the relationship between these parameters is.  
%
Third, in order to handle the tensor-variate functional data, we use vectorization of the model. 
This technique allows to reduce the problem of estimation of an unknown tensor of connection probabilities 
to a solution of a  functional linear regression problem with sub-gaussian errors. The technique is very potent and is used in a novel way. 
In particular, it leads to much more simple mathematics than in Gao \etal (2015)  and Klopp \etal (2017).
In the case of a time-independent SBM, it immediately reduces the SBM to a linear regression setting. 
In addition, by using the properties of the Kronecker product, we are able to reduce   
the  smoothness assumption on the  connection probabilities to sparsity assumption on 
their coefficients in one of the common  orthogonal transforms (e.g., Fourier or wavelet). 
%
Fourth, we use the novel structure of the penalty a part of which is proportional to the logarithm of
the  cardinality of the set of all possible clustering matrices over $L$ time instants.
The latter  allows to accommodate various group membership switching scenarios and is based on 
the Packing lemma (Lemma~4) which can be viewed as a version 
% the Packing lemma (Lemma~\ref{lem:packing}) which can be viewed as a version 
of the Varshamov-Gilbert  lemma for clustering matrices. In particular, while all papers that studied the SBM 
dealt with the situation where no restrictions are placed on the set of clustering matrices,
our approach allows to impose those restrictions.  
%  
Finally,  the methodologies of the paper admit  various generalizations. 
For example, they can be adapted to a situation where the number of nodes 
in the network depends on time, or the connection probabilities have jump discontinuities,  
or when some of the groups have no connection with each other.
%
Section \ref{sec:sparse}  shows that the technique can be adapted to an additional uniform sparsity 
considered in Klopp \etal (2017)  if the number of nodes in each class is large enough.

 

The rest of the paper is organized as follows.
% 
In Section~\ref{sec:notations_data}, we   introduce   the notations and  
describe   the vectorization of the model.
In Section~\ref{sec:assump_inference}, we  construct  the penalized least squares   estimators $\hbLam$
of the tensor $\bLam$. In Section~\ref{sec:oracle},  we derive the oracle inequalities for their risks.
In Section \ref{sec:DSBM_lower bounds}, we obtain the minimax lower bounds for the risk
that confirm that the estimators $\hbLam$ are minimax optimal. 
Section~\ref{sec:sparse} shows how our technique provides 
 adaptation to uniform sparsity assumption studied in Klopp \etal (2017).
Section~\ref{sec:dyn_graphon}    develops the nearly minimax optimal (within a logarithmic factor of $L$) 
estimators of $\bLam$ when the network is generated by a graphon.
Finally,   Section~\ref{sec:discussion}, provides a  discussion of  various generalizations of the techniques 
proposed in the paper. The proofs of all statements   are placed into   the Supplemental Material.

    

  

\section{Notation, discussion of the model   and data structures} 
\label{sec:notations_data}
% \setcounter{equation}{0}


\subsection{Notation} 
\label{sec:notations}

 
% For any positive integer $d$, denote $[d]= \{1,2, \cdots, d\}$.
% For any $a,b \in \RR$, denote $a \vee b = \max(a,b)$, $a \wedge b = \min(a,b)$.
For any two positive sequences $\{ a_n\}$ and $\{ b_n\}$, $a_n \asymp b_n$ means that 
there exists a constant $C>0$ independent of $n$ such that $C^{-1} a_n \leq b_n \leq C a_n$
for any $n$. For any set $\Om$, denote cardinality of $\Om$ by $|\Om|$.
For any $x$, $[x]$ is the largest integer no larger than $x$.
 
 
For any vector $\bt \in \RR^p$, denote  its $\ell_2$, $\ell_1$, $\ell_0$ and $\ell_\infty$ norms by, 
respectively,  $\| \bt\|$, $\| \bt\|_1$,  $\| \bt\|_0$ and $\| \bt\|_\infty$.
Denote by $\|\bt_1 - \bt_2\|_H$ the Hamming distance between vectors $\bt_1$ and $\bt_2$.
Denote by $\bone$ and $\bzero$ the vectors that have, respectively, only unit or zero elements.
Denote by $\boe_j$ the vector with 1 in the $j$-th position and all other elements equal to zero. 




For a matrix $\bA$, its $i$-th row and $j$-th columns are denoted, respectively, by
$\bA_{i, *}$ and $\bA_{*, j}$. Similarly, for a tensor $\bA \in \RR^{n_1 \times n_2\times n_3}$,
we denote its $l$-th $(n_1 \times n_2)$-dimensional sub-matrix by $\bA_{*,*,l}$.
%
Let $\vect(\bA)$ be the vector obtained from matrix $\bA$ by sequentially stacking its columns. Denote  
by $\bA \otimes \bB$ the Kronecker product of matrices $\bA$ and $\bB$. Also, $\bI_k$ is the identity matrix of size $k$.
For any subset $J$ of indices, any vector $\bt$ and any matrix $\bA$, denote the restriction of $\bt$ to indices in $J$ by $\bt_J$
and the restriction of $\bA$ to columns $\bA_{*,j}$ with $j \in J$ by $\bA_J$. Also, denote by $\bt_{(J)}$  the modification of vector 
$\bt$ where all elements $\bt_j$ with $j \notin J$ are set to zero.

% Notation $\bA >0$ or $\bA \geq 0$ means, respectively,  that $\bA$ is positive or non-negative definite.
% Denote determinant of $\bA$ by $|\bA|$ and the largest, in absolute value, element of $\bA$ by $\| \bA\|_{\infty}$.
% Denote the Moore-Penrose inverse of matrix $\bA$ by $\bA^{+}$.
%
 
 

For any matrix $\bA$,  denote its spectral and Frobenius norms by, respectively,  $\| \bA \|_{op}$ and $\| \bA \|$.
Denote 
% Define the Hamming and the $\ell_0$ norms of a matrix $\bA$ by, respectively, 
  $\| \bA \|_H \equiv \| \vect(\bA) \|_H$, $\| \bA \|_\infty = \| \vect(\bA) \|_\infty$ and   $\| \bA \|_0 \equiv \| \vect(\bA) \|_0$.  
For  any tensor $\bA \in \RR^{n_1 \times n_2\times n_3}$, denote 
$\| \bA \|^2 = \sum_{l=1}^{n_3} \|\bA_{*,*,l}\|^2$.


Denote by $\calM (m,n)$ a collection of {\it membership} (or {\it clustering}) matrices $\bZ \in \{0,1\}^{n\times m} $, i.e. 
matrices  such that $\bZ$ has exactly one 1 per row
and $\bZ_{ik} =1$ iff  a node $i$ belongs to the class $\Om_k$ and is zero otherwise.
Denote by $\calC(m,n,L)$  a set of clustering matrices   such that 
\be  \label{clustmatr}
\calC(m,n,L) \subseteq \prod_{l=1}^L \calM (m,n).
\ee  




\subsection{Discussion of the model} 
\label{sec:mod_discuss}


Note that   the values of  $\bB_{i,j,l}$ are independent given the values of $\bLam_{i,j,l}$,
that is,  $\bB_{i,j,l}$ are independent in the sense that their deviations from $\bLam_{i,j,l}$ are independent from each other.
Therefore, the values of  $\bB_{i,j,l}$ are linked to each other in the same way as observations 
of a continuous function with independent Gaussian  errors are related to each other.  
Moreover, in majority of papers treating dynamic block models (see, e.g., Durante \etal (2015), 
Han \etal (2015), Matias and Miele (2017), Yang \etal  (2011) among others), 
similarly  to the present paper, the authors assume that observations $\bB_{i,j,l}$ 
are independent given   $\bLam_{i,j,l}$. Note that this is not an artificial construct: 
Durante \etal (2015), for example, use the model for studying international relationships between countries over time. 


The only difference between the present paper and the papers cited above is that we assume that the underlying 
connection probabilities $\bG_{*,*,l}$ are functionally linked (e.g., smooth) rather than being   
 probabilistically  related. Indeed,   many papers  that treat dynamic block models assume  
some Bayesian generative mechanism on the values of connection probabilities as well as on evolution 
of clustering matrices. In particular, they impose some prior distributions that relate  
 $\bG_{*,*,l+1}$ to $\bG_{*,*,l}$   and $\tilbZ^{(l+1)}$ to $\tilbZ^{(l)}$, the matrices of 
underlying probabilities and the clustering matrices for consecutive time points. 
%
Since the proposed generative mechanism may be invalid, we avoid making assumptions about the probabilistic
structures that generate connection probabilities and group memberships, and treat the network as a given object.
% we do not impose any  specific generative mechanism for the network and treat the network as a given object, 
% we avoid making such assumptions since they may not  be true. 
However, our model enforces, in a sense, a more close but yet flexible relation between the values of $\bB_{i,j,l}$ 
since $\bG_{*,*,l}$ are functionally (and not stochastically) related. 
Moreover, our theory allows to place any restrictions on the set of clustering matrices.


   
To illustrate this point, consider just one pair of nodes  $(i,j)$ and assume that these nodes do not switch their 
memberships between times   $t_l$ and $t_{l+1}$ and also that $\bG_{i,j,l}$ is continuous at $t_l$ . It is easy to see that if  
$\bG_{i,j,l}$ is close to zero (or one), then $\bG_{i,j,l+1}$ is also close to zero (or one) 
and, hence,  $\bB_{i,j,l}$ and $\bB_{i,j,l+1}$ are likely to be equal to zero (or one) simultaneously. 
This relationship takes place in general. 



To simplify the narrative, just for this paragraph, 
denote $\bl = \bB_{i,j,l}$, $\blo = \bB_{i,j,l+1}$,  $\gl = \bG_{i,j,l}$  and $\glo = \bG_{i,j,l+1}$.
In order we are able to assert conditional probabilities $\PP (\bB_{i,j,l+1}=1|\bB_{i,j,l}=1) \equiv \PP(\blo =1|\bl=1)$
and $\PP (\bB_{i,j,l+1}=0|\bB_{i,j,l}=0) \equiv \PP(\blo =0|\bl=0)$, consider the situation 
where $\gl$ and $\glo$ are random variables with the joint pdf $p(\gl, \glo)$ 
such that, given $\gl$, on the average  $\glo$ is equal to $\gl$: $\EE(\glo|\gl) = \gl$.
Assume, as it is done in the present paper,  that, given $g_l$, values of $b_l$ are independent Bernoulli variables, so that 
$$
p(\bl, \blo|\gl, \glo)= \gl^{\bl}(1-\gl)^{1-\bl}\, \glo^{\blo}(1-\glo)^{1-\blo}.
$$
It is straightforward to calculate marginal probabilities $\PP(\bl=1) = \EE(\gl)$,
$\PP(\blo=1) = \EE(\glo) = \EE[\EE(\glo|\gl)] = \EE(\gl)$ and the joint probability
$\PP(\bl =1, \blo =1) = \EE(\glo \gl)= \EE[\EE(\glo \gl|\gl)] = \EE(\gl^2)$ which yields
\bes
\PP(\blo =1|  \bl  =1) - \PP(\blo=1)  = \frac{\EE(\gl^2)}{\EE(\gl)} - \EE(\gl) =  \frac{\Var(\gl)}{\EE(\gl)} >0
\ees
unless $\Var(\gl) =0$. The latter means that, even in the presence of the assumption of the conditional independence, 
the probability of interaction at the moment $t_{l+1}$ is larger if there were an interaction at the moment $t_l$
than it would be in the absence of this assumption. Similarly, repeating the calculation with $\gl$ and $\glo$
replaced by  $1-\gl$ and $1-\glo$, obtain 
\bes
\PP(\blo =0|  \bl  =0) - \PP(\blo=0) = \frac{\Var(\gl)}{\EE(1-\gl)} >0. 
\ees
In the absence of  the probabilistic assumptions on $\gl$ and $\glo$, we cannot evaluate 
those conditional probabilities but the relationship persists in this situation as well.

 



\subsection{Vectorization of the model} 
\label{sec:vectorization}



Note that tensor $\bLam$ of connection probabilities has a lot of structure. On one hand,  
it is easy to check that 
\be \label{eq:bLam}
\bLam_{*,*,l} = \tilbZ^{(l)} \bG_{*,*,l} (\tilbZ^{(l)})^T, \quad \bB_{i,j,l} \sim \mbox{Bernoulli} (\bLam_{i,j,l}),
\ee 
where $\tilbZ^{(l)} \in \calM (m,n)$ is the clustering matrix at the moment $t_l$. On the other hand,
for every $k_1$ and $k_2$, vectors $\bG_{k_1,k_2,*} \in \RR^L$ are comprised of values of some smooth functions
and, therefore, have low  complexity. Usually, efficient representations of such vectors are achieved by applying 
some orthogonal transform  $\bH$   (e.g., Fourier or wavelet transform), however, we cannot apply this transform to the original data 
tensor for two reasons. First, the errors in the model are not Gaussian, so application of $\bH$ will convert 
the data tensor with independent Bernoulli components into a data tensor with dependent entries that are not Bernoulli variables any more.
In addition, application of this transform to the original data will not achieve our goals since, although vectors
$\bG_{k_1,k_2,*}$ represent smooth functions, vectors $\bLam_{i,j,*}$ do not, due to possible switches  in the group memberships.
In addition, for every $l$, matrix  $\bLam_{*,*,l}$ in \fr{eq:bLam} forms the  so called bi-clustering structure 
(see, e.g., Gao \etal (2016)) which makes recovery of $\bG_{*,*,l}$ much harder than in the case of a  usual regression model. 



In order  to handle all these intrinsic difficulties, we apply  operation of vectorization to $\bLam_{*,*,l}$. Denote
\be \label{vec_l}
\blam^{(l)} = \vect(\bLam_{*,*,l}),\quad \bb^{(l)} = \vect(\bB_{*,*,l}),\quad 
\bg^{(l)} = \vect(\bG_{*,*,l}).
\ee
Then, Theorem 1.2.22(i) of Gupta and Nagar (2000) yields 
{\small{
\be  \label{laml_full}
\blam^{(l)} = (\tilbZ^{(l)} \otimes \tilbZ^{(l)}) \bg^{(l)}, \ 
 \bb_i^{(l)} \sim {\rm Bernoulli} (\blam_i^{(l)}), \  i=1, \cdots, n^2,\ l=1, \cdots, L.
\ee 
}}
Note that  $\bb_i^{(l)}$ in \fr{laml_full} are independent for different values of $l$ but not $i$
due to the symmetry. In addition, the values of $\bb_i^{(l)}$ and $\blam_i^{(l)}$
that are corresponding to diagonal elements of matrices $\bB_{*,*,l}$ and   $\bLam_{*,*,l}$, 
are equal to zero by construction. Since all those values are not useful for estimation, 
we remove redundant entries from  vectors $\blam^{(l)}$ and $\bb^{(l)}$ for every $l=1, \cdots, L$. 
Specifically, in \fr{laml_full}, we remove the elements  in $\blam^{(l)}$ and 
the rows in $(\tilbZ^{(l)} \otimes \tilbZ^{(l)})$  corresponding, respectively,  
to   $\bLam_{{i_1},{i_2},l}$ and $(\tilbZ_{i_1,*}^{(l)} \otimes \tilbZ_{i_2,*}^{(l)})$ with $i_1 \geq i_2$.
We denote the reductions of vectors $\blam^{(l)}$, $\bb^{(l)}$ and matrices $(\tilbZ^{(l)} \otimes \tilbZ^{(l)})$
by, respectively, $\bte^{(l)}$, $\ba^{(l)}$ and $\tilbC^{(l)}$ obtaining 
{\small
\be \label{bern2}
\bte^{(l)} = \tilbC^{(l)} \bg^{(l)}, \quad \ba_i^{(l)} \sim {\rm Bernoulli} (\bte_i^{(l)}), \   
i=1, \cdots,   n(n-1)/2,\ l=1, \cdots, L.
\ee 
} 
Note that unlike in the case of $\bb^{(l)}$, elements $\ba_{i}^{(l)}$ and $\ba_{i'}^{(l')}$
are independent whenever $i \neq i'$ or $l \neq l'$. The interesting thing here is that matrices 
$\tilbC^{(l)}$ are still clustering matrices, i.e., $\tilbC^{(l)} \in \calM(n(n-1)/2,m^2)$. Indeed, 
$\tilbC^{(l)}$ are binary matrices such that, 
for  $i$ corresponding to $(i_1, i_2)$ with  $i_1 < i_2$ and $k$ corresponding to $(k_1, k_2)$ 
in $(\tilbZ_{i_1,k_1}^{(l)} \otimes \tilbZ_{i_2,k_2}^{(l)})$ one has 
$\tilbC^{(l)}_{i,k}=1$ if and only if the nodes $i_1 \in \Om_{k_1}$ and  $i_2 \in \Om_{k_2}$.


\begin{figure}
 % \centering
%\[ \hspace*{6mm} \includegraphics[height=6cm]{pic123.eps}  \hspace{3mm}\includegraphics[height=6cm]{pic5.eps}  \]
\[   \includegraphics[height=6.2cm]{Left3.eps}  \hspace{3mm}\includegraphics[height=6cm]{mfig2.eps}  \]
%\[   \includegraphics[height=6cm]{mfig1.eps}  \hspace{3mm}\includegraphics[height=6cm]{mfig2.eps}  \]
%
\caption{\small \label{figure2}   
 Vectorization of the probability tensor $\bLam$ with $n = 4$, $m=2$, $N=n(n-1)/2=6$, $M=m(m+1)/2=3$ and $L=3$.  
 Left panel, top: transforming $\bLam_{*,*,l}$ into $\bte^{(l)}$, $l=1,2,3$. 
 Left panel, middle: $\bLam_{*,*,1} = \tilbZ^{(1)} \bG_{*,*,1} (\tilbZ^{(1)})^T$.  
 Left panel, bottom: $\bte^{(1)} = \tilbC^{(1)} \bg^{(1)} =  \bC^{(1)} \bq^{(1)}$. 
In the left panel, redundant elements of $\bLam$ are white, redundant elements of $\bG$ are yellow. 
 Right panel: $\bte = \bC \bq$. }
\end{figure}





Observe that although we removed the redundant elements from vectors $\blam^{(l)}$ and $\bb^{(l)}$,
we have not done so for the vectors $\bg^{(l)}$. Indeed, since matrices $\bG_{*,*,l}$ are symmetric, 
the elements of vectors  $\bg^{(l)}$ corresponding to $\bG_{k_1,k_2,l}$ and $\bG_{k_2,k_1,l}$ with $k_1 \neq k_2$ are equal 
to each other. For the sake of eliminating such redundancy (and, hence, the need of tracing the equal elements 
in the process of estimation), for indices $k$ corresponding to pairs of classes $(k_1, k_2)$   with $k_1> k_2$, 
we remove entries $\bg^{(l)}_k$  from vectors $\bg^{(l)}$  and denote the resulting vectors by $\bq^{(l)}$. 
In order an equivalent of the relation \fr{bern2} still holds with vectors $\bq^{(l)}$ instead of $\bg^{(l)}$, 
we add together columns of matrices $\tilbC^{(l)}$ corresponding to $(k_1, k_2)$ and $(k_2, k_1)$ with $k_1 < k_2$, obtaining new matrices  $\bC^{(l)}$.
It is easy to see that, for every $l$,  since  $\bC^{(l)}$ is obtained from $\tilbC^{(l)}$ by adding columns together and 
since each row of $\tilbC^{(l)}$ has exactly one unit element with the rest of them being zeros, 
$\bC^{(l)}$ is  again a clustering matrix of size $[n(n-1)/2] \times [m(m+1)/2]$. In particular, for indices $i$ and $k$ corresponding to 
nodes $(i_1,i_2)$ and classes $(\Om_{k_1},\Om_{k_2})$ with $i_1 < i_2$ and $k_1 \leq k_2$, one has $\bC^{(l)}_{i,k}=1$
if  $i_1 \in \Om_{k_1}$ and $i_2 \in \Om_{k_2}$ or $i_1 \in \Om_{k_2}$ and $i_2 \in \Om_{k_1}$;
 $\bC^{(l)}_{i,k}=0$ otherwise. The process of vectorization of the model and removing redundancy is presented in Figure 1.


 
Using $\bC^{(l)}$ and $\bq^{(l)}$, one can rewrite equations \fr{bern2} as  
\begin{align} \label{model_l}
& \ba^{(l)} = \bte^{(l)} + \bxi^{(l)}  \quad \mbox{with} \quad \bte^{(l)} = \bC^{(l)} \bq^{(l)},  \quad l=1, \cdots, L,
% \label{param}
% &  \bte^{(l)} \in \RR^N, \ \bq^{(l)}\in \RR^M,\ \bC^{(l)}\in \calM(M,N).
%\  N=  n(n-1)/2,\ M =  m(m+1)/2   
\end{align}
where $\bC^{(l)}\in \calM(M,N)$,  $\bte^{(l)} \in \RR^N$, $\bq^{(l)}\in \RR^M$, $N=  n(n-1)/2$ and $M =  m(m+1)/2$. 
Here, for every $i$ and $l$, components $\ba_i^{(l)}$ of vector $\ba^{(l)}$ are independent Bernoulli variables with $\PP (\ba_i^{(l)}=1) = \bte_i^{(l)}$, 
so that components of vectors $\bxi^{(l)}$ are also independent for different values of $i$ or $l$.



If we had the time-independent SBM ($L=1$) and the clustering matrix were known, equation \fr{model_l} would reduce 
estimation of $\bq^{(1)}$ to the linear regression problem with independent sub-gaussian (Bernoulli) errors. 
Since in the case of the DSBM, for each $i$, the elements $\bg_i^{(l)}$, $l=1, \cdots, L$, of vector $\bg_i$ 
represent the values of a smooth function, we combine vectors in \fr{model_l} into matrices.
Specifically, we consider matrices $\bA, \bTe, \bXi \in  \RR^{N\times L}$ and $\bQ\in \RR^{M\times L}$
with columns $\ba^{(l)}$, $\bte^{(l)}$, $\bxi^{(l)}$ and $\bq^{(l)}$, respectively. Note that if the group memberships of the nodes were 
constant in time, so that $\bC^{(l)} \in \{0,1\}^{N \times M}$ were independent of $l$, formula \fr{model_l} would imply  
\be \label{con-memb}
\bA = \bTe + \bXi, \quad \bTe = \bZ \bQ 
\quad \mbox{if} \quad \bC^{(l)} = \bZ, \ l=1, \cdots, L.
\ee
However, we consider the situations where nodes can switch group memberships in time and \fr{con-memb} is not true.



For this reason, we proceed with further vectorization. We denote $\ba  = \vect(\bA)$, $\bte = \vect(\bTe)$ and  $\bq = \vect(\bQ)$
and observe that vectors $\ba, \bte \in \RR^{NL}$ and $\bq \in \RR^{ML}$ are obtained 
by stacking  vectors $\ba^{(l)}$, $\bte^{(l)}$ and $\bq^{(l)}$ in  \fr{model_l}   vertically for $l=1, \cdots, L$.  
Define a block diagonal matrix $\bC \in \{0,1\}^{NL \times ML}$ with blocks  $\bC^{(l)}$, $l=1, \cdots, L,$
on the diagonal. Then,   \fr{model_l} implies that
\be \label{full_model}
\ba  = \bte  + \bxi   \quad \mbox{with} \quad \bte  = \bC  \bq = \bC\, \vect(\bQ),
\ee
where $\ba_i$ are independent Bernoulli$(\bte_i)$ variables, $i = 1, \cdots,  NL$.

 
Observe  that if the  matrix $\bC$ were known, then 
equations in \fr{full_model} would represent a regression model with independent
 Bernoulli errors. Moreover, matrix $\bC^T \bC$ is diagonal 
since matrices $(\bC^{(l)})^T \bC^{(l)} = (\bS^{(l)})^2,\ l=1, \cdots, L,$ are diagonal with 
$\bS_{{k_1},{k_2}}^{(l)} = \sqrt{N^{(l)}_{{k_1},{k_2}}}$, where 
$N^{(l)}_{{k_1},{k_2}}$ is the number of pairs $(i_1, i_2)$ of nodes 
such that $i_1 < i_2$ and one node is in class $\Om_{k_1}$ while another is in class 
$\Om_{k_2}$ at time instant $t_l$: 
\be \label{eq:Nkl} 
N^{(l)}_{{k_1},{k_2}} = \lfi 
\begin{array}{ll}
n^{(l)}_{k_1} n^{(l)}_{k_2}, & \mbox{if}\ k_1 \neq k_2;\\
& \\
n^{(l)}_{k_1} (n^{(l)}_{k_1} -1), & \mbox{if}\ k_1 = k_2.
 \end{array} \right.
\ee

\begin{remark} \label{rem:direct}
{\bf (Directed graph). }
{\rm Similar vectorization algorithm can be used when the dynamic  network is constructed 
from directed graphs or  graphs with self loops. In the former case, the only redundant entries 
of matrices $\bLam_{*,*,l}$ would be the   diagonal ones while, in the latter case, $\bLam$ 
has no redundant elements and no row removal is necessary.
} 
\end{remark}



\begin{remark} \label{rem:biclust}
{\bf (Biclustering structures). }
{\rm 
Vectorization presented above can significantly simplify the inference in the so called biclustering
models  considered, for example, by Lee \etal (2010) and Gao \etal (2016). In those models, one needs to recover 
matrix $\bX$ from observations of matrix $\bY$ given by
$\bY = \bU_1 \bX \bU_2 + \bXi$ where matrices $\bU_1$ and $\bU_2$ are known and matrix $\bXi$ has independent 
zero-mean Gaussian or sub-gaussian entries. 
As long as there are no structural assumptions on matrix $\bX$ (such as, e.g., low rank),
one can apply vectorization and reduce the problem to the familiar non-parametric regression problem 
of the form $\by = \bU \bx + \bxi$ where matrix $\bU = \bU_1 \otimes \bU_2$ is known, 
$\bxi = \vect(\bXi)$ is the vector with independent components  
and one needs to recover $\bx = \vect(\bX)$ from observations $\by = \vect(\bY)$. 
}\end{remark} 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Assumptions and estimation for the DSBM}
\label{sec:assump_inference}
% \setcounter{equation}{0}


It is reasonable to assume that the values of the probabilities   $\bq^{(l)}$ 
of connections do not change dramatically from one  time instant   to another.
Specifically, we assume that  for various   $k = 1, \cdots, M$, 
vectors $\bq_{k} = (\bq_{k}^{(1)}, \cdots, \bq_{k}^{(L)})$  
represent   values of some smooth functions, so that $\bq_{k}^{(l)} = f_k (t_l)$, $l=1, \cdots, L$.
%
In order to quantify this phenomenon, we assume that vectors  $\bq_{k}$ have sparse
representation in some orthogonal basis $\bH \in \RR^{L \times L}$ with $\bH^T \bH = \bH \bH^T = \bI_L$,
so that vector  $\bH \bq_{k}^T$ is sparse: it has only few large coefficients, the rest of the coefficients
are small or equal to zero. This is a very common assumption in 
functional data analysis. For example,  if $\bH$ is the matrix of the Fourier transform and    $f_k$
belongs to a Sobolev space  or $\bH$ is a matrix of a wavelet transform and $f_k$ belongs to a Besov space, 
the coefficients $\bH \bq_{k}^T$ of $\bq_{k}^T$   decrease rapidly and, hence, vector $\bH \bq_{k}^T$
is sparse. In particular, one needs only few elements in vector $\bH \bq_{k}^T$ to represent $\bq_{k}$
with high degree of accuracy. The extreme case occurs when the connection probabilities do not change in time,
so that vector $\bq_{k}$ has constant components: then, for the Fourier or a periodic wavelet transform, 
the vector  $\bH \bq_{k}^T$  has only one non-zero element.


Denote $\bD= \bQ \bH^T$ where matrix $\bQ$ is defined in the previous section and $\bd = \vect(\bD)$.
Observe that vector $\bd$ is obtained by stacking together the columns of matrix $\bD = \bQ \bH^T$
while its transpose $\bD^T = \bH \bQ^T$ has vectors $\bH \bq_{k}^T$ as its columns.
% while vector  $\bd$ is obtained by stacking together the columns of its transpose $\bD = \bQ \bH^T$.  
%
Then, sparsity of the matrix $\bD$ can be controlled by imposing a complexity penalty on
$\|\bd \|_0 = \|\bD \|_0 = \|\bD^T\|_0$ on matrix $\bD$. Note that complexity penalty does  not require the actual 
matrix $\bD$ to have only few non-zero elements, it merely forces the procedure to keep only few 
large elements in $\bD$ while setting the rest of the   elements to zero and, hence,  acts as a kind of hard thresholding.
% 
%
Note that by Theorem 1.2.22 of Gupta and Nagar (2000), one has
\be \label{vecQHT}
\bd = \vect(\bQ \bH^T ) = (\bH \otimes \bI_M) \vect(\bQ) = (\bH \otimes \bI_M) \bq = \bW \bq,
\ee 
where   $\bW = (\bH \otimes \bI_M)$   is an orthogonal matrix such
that $\bW^T \bW = \bW \bW^T = \bI_{ML}$. 
% 
Denote  % coefficients of expansion of $\bq$ in $\bW$ by $\bd$ and the set of indices corersponding to nonzero components of $\bd$ by $J$:
\be \label{transformed}
% \bd = \bW \bq, \quad \bd \in \RR^{ML}, \ 
J \equiv J_M = \lfi j:\   \bd_j \neq 0 \rfi,\ \bd_{J^C} = \bzero,
%\quad \bZ = \bC \bW, %\quad \bd \in \RR^{ML}, 
\ee
so that $J$  is the set of indices corresponding to nonzero elements of the vector $\bd$. 



Consider a set of clustering matrices $\calC(m,n,L)$ satisfying \fr{clustmatr}. 
At this point we   impose very mild assumption on $\calC(m,n,L)$: 
\be \label{clust_assump} 
\log(|\calC(m,n,L)|) \geq 2 \log m.
\ee
Assumption \fr{clust_assump} is used just for simplifying expression for the penalty.
Indeed, until now, we allowed any collection of clustering matrices, so potentially, we can 
work with the case where all cluster memberships are fixed in advance (although this would be a totally trivial case).
Condition \fr{clust_assump} merely means that at least two nodes at some point in time can be assigned arbitrarily to any of 
$m$ classes. 
Later,   we shall consider some special cases such as fixed membership (no membership switches over time)  or limited change 
(only at most $n_0$ nodes can change their memberships between two consecutive time points).
\ignore{
Observe that condition \fr{clust_assump} is extremely mild: if there are $n$ nodes that are grouped 
initially into $m$ classes, $n/m$ nodes per class and the group memberships do not change in time,
the inequality \fr{clust_assump} still holds since
\bes
\log(|\calC(m,n,L)|) \geq \log {n \choose n/m} \geq  \frac{n}{m} \log m \geq 2 \log m
\ees
provided $n \geq 2 m$.
} 


We find $m, J, \bd$ and $\bC$  as one of the solutions of the following penalized least squares 
optimization problem
\be \label{opt_problem}
(\hm, \hJ,\hbd, \hbC) = \underset{m,J,\bd,\bC}{\operatorname{argmin}} \lkv \|\ba - \bC \bW^T \bd\|^2 + \Pen(|J|,m)\rkv
\quad \mbox{s.t.}\    \bd_{J^c}=\bzero  
%\arg\min \lfi m,J =J_M,\bd,\bC, \bd_{J^c}=0, \bC \in \calC(m,n,L): 
%\|\ba - \bC \bW^T \bd\|^2 + \Pen(|J|,m)\rfi,
\ee
where $\bC \in \calC(m,n,L)$, $\ba$ is defined in \fr{full_model},  $\bd \in \RR^{ML}$, $\bW \in \RR^{ML \times ML}$, $M=m(m+1)/2$ and 
\be \label{penalty}
\Pen(|J|,m) = 11 \,\log(|\calC(m,n,L)|) +   \frac{11}{2}\,|J| \log\lkr \frac{25\, m^2L}{|J|} \rkr.
\ee
Observe that the penalty in \fr{penalty} consists of two parts. The first part accounts for the complexity
of clustering and, therefore, allows one to obtain an estimator adaptive to the number of unknown groups $m$
as long as the we can express the complexity of clustering in terms of $m,n$ and $L$. The second term represents the price 
of estimating $|J|$ elements of vector $\bd$ and finding those $|J|$ elements in this vector of length $m (m+1) L/2$. 


Note that since minimization is carried out also  with respect to  $m$,
optimization problem \fr{opt_problem}  should be solved separately for every $m =1, \cdots, n$,    
yielding $\hbd_M, \hbC_M$ and  $\hJ_M$. After that, one needs to select   
the value $\hM = \hm(\hm+1)/2$ that delivers the minimum in 
\fr{opt_problem}, so that
\be \label{est_val}
\hbd = \hbd_{\hM},\quad \hbC = \hbC_{\hM}, \quad \hJ = \hJ_{\hM}.
\ee 
Finally, due to \fr{transformed}, we  set  $\hbW = (\bH \otimes \bI_{\hM})$ and calculate 
\be \label{est}
 \hbq = \hbW^T \hbd,\quad  \hbte = \hbC \hbq.
\ee
We obtain $\hbLam$ by packing vector $\hbte$ into the  tensor and taking the symmetries into account.


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Oracle inequalities for the DSBM}
\label{sec:oracle}
% \setcounter{equation}{0}


Denote the true value of tensor $\bLam$ by $\bLams$. Also, denote by $\ms$ the true number of groups, 
by $\bqs$ and  $\btes$ the true values of $\bq$   and $\bte$ in \fr{full_model}
and by  $\bCs$ the true value of $\bC$. Denote by $\bDs$ and $\bds$ the true 
values of matrix $\bD$ and vector $\bd$, respectively.
Let $\Ms = \ms(\ms+1)/2$ and $\bWs = (\bH \otimes \bI_{\Ms})$ 
be true values of $M$ and $\bW$.
% 
Note that vector $\btes$ is obtained by vectorizing  $\bLams$ 
and then removing the redundant entries. Then, it follows from \fr{full_model} that 
\be \label{true_model}
\ba  = \btes  + \bxi   \quad \mbox{with} \quad \btes  = \bCs  \bqs = \bCs (\bWs)^T \bds.
\ee
Due  to the relation between the $\ell_2$ and the Frobenius norms, one has  
% for any vector $\bte$ and any tensor $\bLam$, one has 
\be \label{symrel}
\|\bte - \btes\|^2 \leq \|\bLam - \bLams\|^2 \leq 2 \|\bte - \btes\|^2,   
\ee 
%
and the following statement holds.

 

\begin{theorem} \label{th:oracle}
Consider a DSBM with a true matrix of probabilities $\bLams$ and the estimator $\hbLam$ obtained according to 
\fr{opt_problem}--\fr{est}. Let $\calC(m,n,L)$ be a set of clustering matrices satisfying conditions 
\fr{clustmatr} and \fr{clust_assump}.
%
Then, for any $t>0$, with probability at least $1 - 9 e^{-t}$, one has
{\small
\be \label{oracle_prob} 
\frac{\|\hbLam - \bLams\|^2}{n^2\,L} \leq \min_{\stackrel{m,J,\bd}{\bC \in \calC(m,n,L)}}
\lkv     \frac{6\, \| \bC \bW^T \bd_{(J)} - \btes \|^2}{n^2\,L} +  \frac{4\, \Pen(|J|,m)}{n^2\,L} \rkv + 
\frac{38\, t}{n^2\,L}  
\ee
}
 and 
{\small
\be \label{oracle_expec}
\EE  \lkr   \frac{\|\hbLam - \bLams\|^2}{n^2\,L}  \rkr \leq \min_{\stackrel{m,J,\bd}{\bC \in \calC(m,n,L)}}
\lkv  \frac{6\, \| \bC \bW^T \bd_{(J)} - \btes \|^2}{n^2\,L} +  \frac{4\, \Pen(|J|,m)}{n^2\,L}   + \frac{342}{n^2\,L}\rkv,
\ee}
%
where $\bd_{(J)}$ is the modification of vector 
$\bd$ where all elements $\bd_j$ with $j \notin J$ are set to zero.
\end{theorem}
 
 

The proof of   Theorem~\ref{th:oracle} is given in the Supplementary Material. % Section \ref{sec:suppl}.
Here, we just explain its idea. 
%
Note that if the values of $m$ and $C$ are fixed, the problem \fr{opt_problem}
reduces to a regression problem with a   complexity penalty $\Pen(|J|,m)$.
Moreover, if $J$ is known, the optimal estimator $\hbd$ of $\bd^*$ is just a projection estimator.
Indeed, denote $\bUp_{\bC} = \bC \bW^T$ and let $\bUp_{\bC,J} = (\bC \bW^T)_J$ be the reduction of matrix $\bC \bW^T$ to columns $j \in J$.
Given $\hm$, $\hJ$ and $\hbC$, one obtains $\hM = \hm(\hm+1)/2$,
$\hbW = (\bH \otimes \bI_{\hM})$,  $\bUp_{\bC,J} = (\bC \bW^T)_J$
%  $\hbUp = \hbC \hbW^T$ 
and  $\hbUp_{\hbC,\hJ} = (\hbC \hbW^T)_{\hJ}$.
Let  
\be \label{projection}
\PJ=   \bUp_{\bC,J} (\bUp_{\bC,J}^T \bUp_{\bC,J})^{-1} \bUp_{\bC,J}^T,\  % \quad \mbox{and} \quad
% 
\hPhJ =  \hbUphJ (\hbUphJ^T \hbUphJ)^{-1} \hbUphJ^T
\ee 
be the projection matrices on the column spaces of $\bUp_{\bC,J}$ and $\hbUphJ$, respectively. 
Then, it is easy to see that $\hbUphJ\, \hbd = \hPhJ\, \ba$ and vector $\hbd$ is of the form 
%
\be \label{hbd_solution}
\hbd = (\hbUphJ^T \hbUphJ)^{-1}\, \hbUphJ^T\, \ba.
\ee
Hence, the values of $\hm$, $\hJ$ and $\hbC$ can be obtained as a solution of the following optimization problem
\bes 
(\hbC, \hm, \hJ) = \underset{m,J, \bC}{\operatorname{argmin}} \lkv \|\ba - \PJ \ba \|^2 + \Pen(|J|,m)\rkv
\quad \mbox{s.t.}\  % J \equiv J_M,\ %\bd_{J^c}=0,\ 
\bC \in \calC(m,n,L),
% \arg\min \lfi m,J =J_M, \bC,   \bC \in \calC(m,n,L): 
%\|\ba - \PhJ \ba \|^2 + \Pen(|J|,m)\rfi,
\ees 
where $\PJ$ and   $\Pen(|J|,m)$  are defined in \fr{projection} and \fr{penalty}, respectively. 
After that, we use the    arguments that are relatively standard in the proofs of oracle inequalities 
for the penalized least squares estimators.
 
  
   
Note that $\| \bC \bW^T \bd_{(J)} - \btes \|^2$ in  the right-hand sides of expressions \fr{oracle_prob} and \fr{oracle_expec},
 is the  bias term that quantifies how well one can estimate the true values of probabilities $\btes$ 
by   blocking them together, averaging the values in each block and simultaneously
setting all but $|J|$ elements of vector $\bd$ to zero. 
If $|J|$ is too small, then $\bd$ will not be well represented by its truncated version 
$\bd_{(J)}$ and the bias will be large. 
% coefficients of the expansions of the vector $\bq$ in \fr{full_model} in the basis $\bW$ to zero.
The penalty represents the stochastic error and constitutes the ''price'' for choosing too many blocks and coefficients.
In particular, the second term  $(11/2)\  |J| \log\lkr  25\, m^2L/|J|  \rkr$
% $(n^{2} L)^{-1}\, |J| \, \log(4 m^2L/|J|)$ 
in \fr{penalty} is due to the need of finding and estimating  $|J|$ 
elements of the $L m(m+1)/2$-dimensional vector. 
The  first term, $\log(|\calC(m,n,L)|)$, accounts for the difficulty of clustering
and is due to application of the union bound in probability. 
 


Theorem \ref{th:oracle} holds for any collection  $\calC(m,n,L)$ of clustering matrices satisfying assumption \fr{clust_assump}. 
In order to obtain some specific results, denote by $\calZ (m,n,n_0,L)$ the collection of clustering matrices 
corresponding to the situation where   at most $n_0$ nodes can change their memberships between any two consecutive time points,
so that 
\be \label{card_clust}
|\calZ (m,n,n_0,L)| = m^n \lkv {n \choose n_0} m^{n_0} \rkv^{L-1}, 
% |\calZ (m,n,0,L)|= m^n; \quad |\calZ (m,n,n,L)| = m^{nL}.
\ee
yielding $|\calZ (m,n,0,L)|= m^n$ and $|\calZ (m,n,n,L)| = m^{nL}$.
Note that the case of $n_0=0$ corresponds to the scenario where  the group memberships of the nodes are 
constant and do  not depend on time while the case of $n_0=n$ means that memberships of all nodes can 
change arbitrarily from one time instant to another. 
% Then, one obtains the following corollary of Theorem \ref{th:oracle}.
Since 
\bes
\log \lkv {n \choose n_0} m^{n_0} \rkv \leq n_0  \,\log\lkr \frac{mne}{n_0}\rkr,  
\ees
formulae \fr{penalty} and  \fr{card_clust} immediately yield the following corollary.


\begin{corollary} \label{cor:upper_DSBM}
Consider a DSBM with a true matrix of probabilities $\bLams$ and estimator $\hbLam$ obtained according to 
\fr{opt_problem}--\fr{est} where $\calC(m,n,L) = \calZ (m,n,n_0,L)$. Then, inequalities \fr{oracle_prob} and \fr{oracle_expec} hold with 
\be \label{specific_pen}
\small{
\Pen(|J|,m) = 11 \lkv n \log m +   n_0 (L-1)\log\lkr \frac{mne}{n_0}\rkr + \frac{|J|}{2}  \log\lkr \frac{25\, m^2L}{|J|}\rkr \rkv
}
% \frac{\Pen(|J|,m)}{n^2\,L} = \frac{11\, \log m}{nL} + 
% \frac{11\, n_0 (L-1)}{n^2 L} \,\log\lkr \frac{mne}{n_0}\rkr + \frac{11\,  |J|}{2 n^2 L}\,  \log\lkr \frac{25\, m^2L}{|J|}\rkr  
\ee
\end{corollary}
 
It is easy to see that the first term in \fr{specific_pen} accounts for  the uncertainty of the initial clustering,
the second term is due to the changes in the group memberships of the nodes over time (indeed, if $n_0 =0$, this term just vanishes)
while the last term is identical to the second term in the expression for the generic penalty \fr{penalty}.
While we elaborate only on the special case where the collection of clustering matrices is given by \fr{card_clust},
one can easily produce results similar to Corollary \ref{cor:upper_DSBM} for virtually any nodes' memberships scenario. 
  
 
  
 
\begin{remark} \label{rem:SBM}
{\bf (The SBM). }
{\rm Theorem \ref{th:oracle} provides an oracle inequality in the case of 
a   time-independent  SBM ($L=1$). Indeed, in this case, by taking $\bH = 1$ and $\bW = \bI_M$, obtain
for any $t>0$
\beqn  \label{oracle_probL1}
\frac{\EE  \|\hbLam - \bLams\|^2}{n^2} & \leq  & \min_{\stackrel{m,J,\bq}{\bC \in \calM(m,n)}}
\lkv     \frac{6  \| \bC \bq_{(J)} - \btes \|^2}{n^2} +  \frac{44 \, \log m}{n} \right. \\
& + &
\left. \frac{22 |J|}{n^2}  \log \lkr\frac{25\, m^2}{|J|}\rkr \rkv + \frac{342}{n^2}\nonumber  
\eeqn  
and a similar result holds for the probability.
Note that if $|J|=m(m+1)/2$, our result coincides with the one of Gao \etal (2015). However, if many groups 
have zero probability of connection, then   $|J|$ is small and the right-hand of \fr{oracle_probL1} 
can be asymptotically smaller than $n^{-1}\, \log m  + n^{-2} m^2$
obtained in  Gao \etal (2015). In addition, our oracle inequality is non-asymptotic 
and the  estimator is  naturally adaptive to the unknown number of classes.
(Gao \etal (2016) obtained adaptive estimators but not via an oracle inequality).
}
\end{remark}


Corollary \ref{cor:upper_DSBM} quantifies the stochastic error term in Theorem \ref{th:oracle}. 
The size of the bias depends on the level of sparsity of coefficients  of functions $\bq_k$ in the basis $\bH$
and on the constitution of classes. While one can study a variety of scenarios, in order to be specific,
we consider the case   of a {\it balanced network model} where the sizes of all the classes are proportional to each other, 
in particular, for some absolute constants $0 < \aleph_1 \leq 1 \leq  \aleph_2 < \infty$, one has
\be \label{balanced}
\aleph_1\, \frac{n}{m} \leq n_k^{(l)} \leq \aleph_2 \, \frac{n}{m}, \quad  k=1, \ldots, m,\ l=1, \ldots, L,
\ee
where $n_k^{(l)}$ the number of nodes in class $k$ at the moment $t_l$. 



Note that the condition \fr{balanced} is very common in studying random network models
(see, e.g.,  Gao \etal (2017) or Amini and Levina  (2018)  among others). 
In addition, if class memberships are generated from the multinomial distribution
with the vector of probabilities $(\pi_1, \cdots, \pi_m)$,  
and $C_1/m \leq \pi_i \leq C_2/m$ for some constants $0<C_1<C_2<\infty$,
as it is done in, e.g., Bickel and Chen  (2009), condition   \fr{balanced} holds with high probability. 



In particular, we consider networks that satisfy condition \fr{balanced} but yet allow only $n_0$ nodes 
switch their memberships between time instances. We denote the corresponding set of clustering matrices 
by $\calZ_{\bal} (m,n,n_0,L,\aleph_1, \aleph_2)$. It would seem that condition \fr{balanced} should make clustering much simpler. 
However, as  Lemma \ref{lem:balanced} below shows, this reduction does not makes estimation significantly easier since
the complexity  of the set of balanced clustering matrices $\log |\calZ_{\bal} (m,n,n_0,L,\aleph_1,\aleph_2)|$ 
is smaller than the complexity  of the set of unrestricted clustering matrices  $\log |\calZ (m,n,n_0,L)|$
only by, at most, a constant factor.
 

\begin{lemma} \label{lem:balanced} {\bf(Balanced network model complexity)}
% {\bf(Complexity of the balanced model).}
If $n \geq \sqrt{e\,n_0^3}$, then 
\be \label{card_balanced}
 % \frac{1}{4}  \lkr n \log m +  n_0  \,\log\lkr \frac{mne}{n_0}\rkr \rkr \leq 
%\log |\calZ_{\bal} (m,n,n_0,L,\aleph_1,\aleph_2)| \leq n \log m +  (L-1)\, n_0  \,\log\lkr \frac{mne}{n_0}\rkr.
\log |\calZ_{\bal} (m,n,n_0,L,\aleph_1,\aleph_2)| \geq \frac{1}{4} \lkv n \log m +  (L-1)\, n_0  \,\log\lkr \frac{mne}{n_0}\rkr \rkv.
\ee 
\end{lemma}
 
 
% We denote by $\calK_M$ the set of pairs of nodes that have non-zero connection probability.
% such that the rows $\bQs_{k,*}$, $k=1, \cdots, M$, of matrix  $\bQs$ 
% (and, therefore, the rows $\bDs_{k,*}$, $k=1, \cdots, M$, of matrix  $\bDs$) are not equal to identical zero.
% Let $M_0 = |\calK_M|$ be the cardinality of this set  (with $M_0=M$ if all groups in the network communicate with one another).
Then, one can use the same penalty that was considered in Corollary  \ref{cor:upper_DSBM}, so that Theorem~\ref{th:oracle}  yields the following result.


\begin{theorem} \label{th:upper_smooth_DSBM}
Consider a balanced DSBM satisfying condition \fr{balanced}. 
% Let $\calK^{*}_{\Ms}$ be the set of pairs of nodes such that $\bDs_{k,*} \equiv \bzero$ if $k \not\in \calKsMs$. 
Let  $\bLams$ be the true matrix of probabilities,  $\ms$ be the true number of classes, $\Ms = \ms(\ms +1)/2$, $\bQs$ be
the true matrix of probabilities of connections for pairs of classes and $\bDs = \bQs \bH$.
%
If $n \geq \sqrt{e\,n_0^3}$ and the  estimator $\hbLam$ is obtained as a solution of optimization problem \fr{opt_problem} with 
the penalty \fr{specific_pen} where  
\be \label{eq:J_union}
J = \bigcup_{k=1}^M J_k,
\ee 
then, for any $t>0$, with probability at least $1 - 9 e^{-t}$, one has
\be \label{oracle_prob_specific} 
{\small \frac{\|\hbLam - \bLams\|^2}{n^2 L} \leq  
 \min_{J} \lfi   \frac{6 \aleph_2^2}{(\ms)^2 L}   \sum_{k=1}^{\Ms}   \sum_{l \notin J_k} (\bDs_{k,l})^2  +  \frac{4  \Pen(|J|,\ms)}{n^2 L} \rfi  + 
\frac{38  t}{n^2 L} }
% \min_{J} \lfi   \frac{6 \aleph_2^2}{(\ms)^2\,L}\,  \sum_{k=1}^{\Ms} \, \sum_{l \notin J_k} (\bDs_{k,l})^2  +  \frac{4\, \Pen(|J|,\ms)}{n^2\,L} \rfi  + 
% \frac{38\, t}{n^2\,L} } 
\ee
 and a similar result holds for the expectation. 
\end{theorem}



In order to obtain  specific upper bounds in \fr{oracle_prob_specific}, we need to impose some assumptions on 
the smoothness of functions $\bQ_{k, *}^*$, $k=1, \cdots, \Ms$.
%
For  the sake of brevity, we assume that all   vectors  $\bDs_{k,*},$ $k=1, \cdots, \Ms$, behave similarly with respect to the basis $\bH$ 
(generalization to the case where this is not true is rather pedestrian but very cumbersome
as we point out in Section \ref{sec:discussion}, Discussion).
\\

{\bf (A0). } There exist   absolute constants $\nu_0$ and $K_0$ such that  
\be \label{bias_coef_cond}
\sum_{l=1}^L (l-1)^{2 \nu_0}\, (\bDs_{k,l})^2     \leq   K_0, \quad k =1, \cdots, \Ms. 
\ee  
%




\begin{corollary} \label{cor:upper_smooth_DSBM}
Let conditions of Theorem \ref{th:upper_smooth_DSBM} hold and 
$\bDs_{k,*}$ satisfy assumption \fr{bias_coef_cond}.   
If  the  estimator $\hbLam$ is obtained as a solution of optimization problem \fr{opt_problem}, 
then for any $t>0$, with probability at least $1 - 9 e^{-t}$, one has
\begin{align}  \nonumber 
\frac{\|\hbLam - \bLams\|^2}{n^2\,L} & \leq \tilde{K}_0\, \lkr  
\min \lfi \frac{1}{L}\, \lkv \lkr \frac{\ms}{n}\rkr^2 \, \log \lkr \frac{n}{\ms} \rkr \rkv^{\frac{2\nu_0}{2 \nu_0 +1}},
\lkr \frac{\ms}{n}\rkr^2 \rfi \right. \\
& + \left.  \frac{\log \ms}{nL} + \frac{n_0}{n^2} \log \lkr \frac{\ms\, n e}{n_0} \rkr + \frac{t}{n^2 L} \rkr
\label{oracle_prob_smooth}
 \end{align}
 and a similar result holds for the expectation. Here, $\tilde{K}_0$ is an absolute constant that depends on 
$K_0$, $\nu_0$, $\aleph_1$ and $\aleph_2$ only.
\end{corollary}
 

\ignore{ 
\begin{remark} \label{rem:VarCoef}
{\bf (Relation to the sparse varying coefficient model). }
{\rm  If $m=n$, the model studied in this paper reduces to the sparse varying coefficient model studied in, e.g., 
Klopp and Pensky  (2015). Note, however, that the problem studied in this paper is much more difficult. To start with, 
there is a block structure imposed on all variables. In addition, the number of blocks is unknown and 
the block memberships of the nodes are unknown and  can change in time. To appreciate the degree of the added complexity, compare the paper of 
Gao \etal (2015) to a paper on a well known classical regression (and Gao \etal (2015) does not have an added difficulty
of block memberships  switching in time and the number of blocks being unknown). 
}
\end{remark}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The lower bounds for the risk  for the DSBM}
\label{sec:DSBM_lower bounds}
% \setcounter{equation}{0}


In order to prove that the estimator obtained as a solution of optimization problem 
\fr{opt_problem} is minimax optimal, we need to show that  the upper bounds in Corollaries~\ref{cor:upper_DSBM}
and \ref{cor:upper_smooth_DSBM} coincide with the minimax lower bounds obtained under similar constraints. 
%
For the sake of derivation of   lower bounds for the error,   we impose   mild conditions on   
the orthogonal   matrix $\bH$  as follows: for any binary vector
$\bom \in \{0,1\}^L$ one has 
\be \label{H_assump}
\|\bH^T \bom \|_\infty \leq \|\bom\|_1 /\sqrt{L} \quad \mbox {and}\quad
\bH \, \bone = \sqrt{L} \boe_1,
%\quad  \bone = (1,1, \cdots, 1)^T,\ \boe_1 = (1,0, \cdots, 0)^T.
\ee
where $\bone = (1,1, \cdots, 1)^T$ and $\boe_1 = (1,0, \cdots, 0)^T$.
%
Assumptions \fr{H_assump} are not restrictive. In fact, they are satisfied for a variety of common 
orthogonal transforms such as the Fourier transform or a periodic wavelet transforms.  


%


First, we derive the lower bounds for the risk under the assumption that vector $\bd$ is $l_0$-sparse
and has only $s$ nonzero components. 
%
Let $\calG_{m,L,s}$ be a collection of tensors such that $\bG \in  \calG_{m,L,s}$ 
implies that the vectorized versions $\bq$ of $\bG$ can be written as 
$\bq = \bW^T \bd$ with $\|\bd\|_0 \leq s$. 
In order to be more specific, we consider the collection of clustering matrices 
$\calZ (m,n,n_0,L)$ with cardinality given by \fr{card_clust} that corresponds to the situation 
where   at most $n_0$ nodes can change their memberships between consecutive time instants.
In this case, $\Pen(|J|,m)$ is defined in \fr{specific_pen}.

 

\begin{theorem} \label{th:DSBM_lower_bounds}
Let orthogonal matrix $\bH$ satisfy condition \fr{H_assump}.
Consider the DSBM   where $\bG \in  \calG_{m,L,s}$  with $s \geq \kappa m^2$ where $\kappa>0$ is 
independent of $m$, $n$ and $L$. Denote $\ga = \min(\kappa, 1/2)$ and assume that 
%
$L \geq 2$, $n \geq 2m$, $n_0 \leq \min(\ga n, 4/3\, \ga n\,m^{-1/9})$ and $s$ is such that
\be \label{Js_cond}
s^2 \log(2 L M/s) \leq 68 L M n^2.
\ee 
Then
\begin{align} \label{lower_DSBM}
\inf_{\hbLam} \sup_{\stackrel{\bG \in \calG_{m,L,s}}{\bC \in \calZ (m,n,n_0,L)}}
\PP_{\bLam} & \lfi \frac{\|\hbLam - \bLam \|^2}{n^2\,L} \right.   \geq   C(\ga)   \lkr 
\frac{\log m}{n L} + \frac{n_0}{n^2}  \log \lkr \frac{m n e}{n_0} \rkr    \right.\\
%\frac{\log|\calZ (m,n,n_0,L)|}{n^2 L}
&+   \left. \left. \frac{s\, \log (L  m^2/s)}{n^2 L} \rkr \rfi \geq \frac{1}{4}, \nonumber
\end{align}
 where $\hbLam$ is any estimator of $\bLam$, $\PP_{\bLam}$ is the probability under the true value 
of the tensor $\bLam$ and $C(\ga)$ is an absolute constant that depends on $\ga$ only.
% and $|\calZ (m,n,n_0,L)|$ is provided in  \fr{card_clust}.
 \end{theorem}



Theorem \ref{th:DSBM_lower_bounds} ensures that if vector $\bd$ has only $s$ nonzero components,  
then the upper bounds in Corollary~\ref{cor:upper_DSBM} are optimal up to a constant.
In order to provide a similar assertion in the case of Corollary \ref{cor:upper_smooth_DSBM}, we assume that rows of  matrix $\bD$ are 
$l_2$-sparse. For this purpose, we consider a collection of tensors $\calG_{m,L,\nu_0}$ such that $\bG \in  \calG_{m,L,\nu_0}$ 
implies that $\bQ = \bD \bH$ and rows $\bD_{k,*}$ of matrix $\bD$ satisfy condition \fr{bias_coef_cond}.
Let as before $\calZ_{\bal} (m,n,n_0,L,\aleph_1, \aleph_2)$ be a collection of clustering matrices 
satisfying condition \fr{balanced} and  such that at most $n_0$ nodes change their memberships between two 
consecutive time instances. The following statement ensures that the upper bounds in Corollary \ref{cor:upper_smooth_DSBM}
are minimax optimal up to a constant factor.
  

\begin{theorem} \label{th:smooth_DSBM_lower_bounds}
Let orthogonal matrix $\bH$ satisfy condition \fr{H_assump}.
Consider the DSBM   where $\bG \in  \calG_{m,L,\nu_0}$  with $\nu_0 > 1/2$,
$L \geq 2$ and  $n \geq 2m$. 
Then, for any absolute constants $0 < \aleph_1 \leq 1 \leq  \aleph_2 < \infty$, one has
\begin{align} \label{smooth_lower_DSBM}
\inf_{\hbLam} \sup_{\stackrel{\bG \in \calG_{m,L,s}}{\bC \in \calZ_{\bal}}}
\PP_{\bLam} & \lfi \frac{\|\hbLam - \bLam \|^2}{n^2\,L} \right.   \geq 
%  
   C\, \lkv \min \lfi \frac{1}{L} \lkv \lkr \frac{m}{n}\rkr^2  \rkv^{\frac{2 \nu_0}{2 \nu_0+1}};   
\lkr \frac{m}{n}\rkr^2 \rfi \right. \\
& \left. \left.  + \frac{\log m}{nL} + \frac{n_0}{n^2}  \log \lkr \frac{m n e}{n_0} \rkr \rkv \rfi \geq \frac{1}{4},
\nonumber
\end{align}
 where $\calZ_{\bal}$ stands for $\calZ_{\bal} (m,n,n_0,L,\aleph_1, \aleph_2)$,
$\hbLam$ is any estimator of $\bLam$, $\PP_{\bLam}$ is the probability under the true value 
of the tensor $\bLam$ and $C$ is an absolute constant independent of $n$, $m$ and $L$.
 \end{theorem}
  


%
Theorems~\ref{th:DSBM_lower_bounds} and \ref{th:smooth_DSBM_lower_bounds} 
confirm that the estimator constructed above is {\bf minimax optimal up to a constant}
if $\bG \in \calG_{m,L,s}$ and $\bC \in \calZ (m,n,n_0,L)$, or $\bG \in  \calG_{m,L,\nu_0}$
and $\bC \in \calZ_{\bal} (m,n,n_0,L,\aleph_1, \aleph_2)$.   
 

Note that the   terms $\log m/(nL)$  and $n_0 n^{-2}\, \log(mne/n_0)$   in \fr{lower_DSBM}   and \fr{smooth_lower_DSBM}
correspond  to, respectively, the error of initial clustering and the clustering error due to membership changes.
The remaining  terms are due to   nonparametric estimation and model selection.
%
Assumptions \fr{H_assump} and \fr{Js_cond} are purely technical and are necessary to 
ensure that  the ``worst case scenario'' tensor $\bG$ 
of connection probabilities has nonnegative components. As we mentioned earlier, 
conditions \fr{H_assump} are totally non-restrictive. 
%
Condition \fr{Js_cond} in Theorem~\ref{th:DSBM_lower_bounds} holds 
whenever representation of the tensor of probabilities in the basis $\bH$ is at least somewhat sparse.
Indeed, if there is absolutely no sparsity (which is a very implausible  scenario when smooth functions are represented in a basis) 
and $s \approx M L$, then condition \fr{Js_cond} reduces to  $m (m+1) L   \leq C n^2$ and will still be true if $L$ is relatively small. 
If $L$ is large, the situation where $s \approx M L$ is very unlikely. 
%
Assumption that $s \geq \kappa m^2$ for some  $\kappa>0$ independent of  
$m$, $n$ and $L$,  restricts the sparsity level and ensures that one does not 
have too many classes where nodes have no interactions with each other or members
of other classes.  



Finally, it is also worth  keeping in mind that 
all assumptions in Theorems~\ref{th:DSBM_lower_bounds} and \ref{th:smooth_DSBM_lower_bounds}   are used for the derivation 
of the minimax lower bounds for the risk and are not necessary
for either the  construction of the estimator $\hbLam$ of $\bLam$ in \fr{opt_problem} 
 or for the assessment of its  precision in Theorems~\ref{th:oracle} and \ref{th:upper_smooth_DSBM}.


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{The uniformly sparse DSBM}
% \section{Sparse DSBMs}
\label{sec:sparse}



In the current literature, the notion of the sparse SBM  refers to the case where the entries of the matrix 
of the connection probabilities are uniformly small: $\bLam = \rho_n \bLam^{(0)}$ with $\| \bLam^{(0)} \|_\infty =1$ and 
$\rho_n \to 0$ as $n  \to \infty$. The concept is based on the idea that when the number of nodes in a network grow,
the probabilities of connections between them decline. The minimax study of the sparse SBM has been carried out
by Klopp \etal (2017). The logical generalization of the sparse SBM of this type would be the sparse DSBM where
the elements of the tensor $\bLam$ are bounded above by $\rho_n$ where  $\rho_n \to 0$ as $n$ grows.
We refer to this kind of network as {\it uniformly sparse}. 
% 


On the other hand,  not all networks become uniformly sparse as  $n  \to \infty$.
Indeed, in the real world, when a  network grows, the number of communities increase and, while 
the probabilities of connections for majority of pairs of groups become very small, some of the of pairs groups 
will still maintain  high connection probabilities. We refer to this type of network as {\it non-uniformly sparse}.  
 The idea of such a  network has been  elaborated in the recent paper of
Borgs \etal (2016). The authors  considered  heavy-tailed sparse graphs such that, in the context of the SBM, 
one still has $\bLam = \rho_n \bLam^{(0)}$ but the elements of $\bLam^{(0)}$ are no longer bounded by one
but by a quantity that grows with $n$.



While distinguishing between very small probabilities might be essential in a clustering problem,
it is not so necessary in the problem of estimation of the tensor of the connection probabilities 
studied in the present paper. Indeed, it is a common knowledge that, in the nonparametric regression model, 
in order to obtain the best error rates, one needs to replace   small elements of the vector of interest 
by zeros rather than estimating them. Similarly, if the network is non-uniformly sparse, i.e., some pairs of
groups have probabilities of connections  equal or very close to zero, one would obtain an estimator with   
better overall precision by setting those very small connection probabilities to zeros.   
%
Although nowhere in the present paper  we   make  an assumption that a network is sparse and, moreover, consideration of 
the non-uniformly sparse SBM or DSBM is not one of its objectives, the paper naturally provides the tools
for minimax optimal statistical estimation  in such models that deliver results with very little  
additional work.
 



In addition, the techniques developed in this paper allow, with some additional work, to extend results obtained 
in Klopp \etal (2017) to the dynamic setting. However, majority of their results depend upon solution of 
optimization problem \fr{opt_problem} under the restriction that $\|\bW^T \bd\|_{\infty} \leq \rho_n$ 
which requires representation of the estimator via a different projection operator and will result in more cumbersome calculations. 
Therefore, we avoid studying this new optimization problem and only extend Corollary 2.2    of 
Klopp \etal (2017)  that handles the case of the balanced model without placing the above-mentioned restriction.  
For this purpose, consider a small $\rho_n$  and denote
\be \label{eq:rnm}
r_n(m) = \max(\rho_n, m^2/n^2).
\ee
% 
Similarly to \fr{opt_problem}, we find $m, J, \bd$ and $\bC$  as one of the solutions of the following penalized least squares 
optimization problem
{\small 
\be \label{opt_problem_balanced}
 (\hm, \hJ,\hbd, \hbC) = \underset{m,J,\bd,\bC}{\operatorname{argmin}} \lkv \|\ba - \bC \bW^T \bd\|^2 +  \lam_0  r_n(m)   \Pen(|J|,m)\rkv
\  \mbox{s.t.}\    \bd_{J^c}=\bzero    
\ee}

\noindent
where $\bC \in \calZ_{\bal} (m,n,n_0,L,\aleph_1,\aleph_2)$, $\ba$ is defined in \fr{full_model},  
$\bd \in \RR^{ML}$, $\bW \in \RR^{ML \times ML}$, $M=m(m+1)/2$, $\Pen(|J|,m)$  is defined in \fr{specific_pen}
and $\lam_0$ is a tuning parameter that is bounded above and below by a constant.
\\

In order the estimator has the uniform sparsity property, we need to make sure that transformation $\bH$ is such that, 
whenever it is used for sparse representation of smooth functions, the maximum absolute value of the estimator  
obtained by truncation of the vector of coefficients is bounded above by a constant factor of the  
maximum absolute value of the original function. In particular, we denote the projection matrix 
on the column space of matrix $(\bC \bW^T)_J$ by $\PCJ$ and impose the following condition on the transformation matrix $\bH$:
\\

{\bf (A1). }   There exists an absolute  constant $B_0$  such that  for any 
$\bC \in \calZ_{\bal} (m,n,n_0,L,\aleph_1,\aleph_2)$ and any vector $\bte$ 
\be \label{uni_sparse_cond}
\| \PCJo \bte \|_\infty = \| \te - \PCJ \te \|_\infty \leq B_0 \| \te\|_\infty.
\ee


% Note that assumption \fr{uni_sparse_cond}  always holds in the case of $L=1$. 
 
 
Let, as before, $\bLams$ be the true matrix of probabilities,    $\ms$  be the true number of classes, 
$\Ms = \ms(\ms +1)/2$, $\bCs$  be the true clustering matrix, $\bQs$  be the true matrix of 
probabilities of connections for pairs of classes, $\bDs = \bQs \bH$, $\bds = \vect(\bDs)$,
$\btes = \bCs (\bWs)^T \bds$ and $\bWs = \bH \otimes \bI_{\Ms}$. 
%  


\begin{theorem} \label{th:sparse}
Consider a balanced DSBM satisfying condition \fr{balanced}.  Let matrix $\bH$ be such that  condition 
\fr{uni_sparse_cond} is satisfied  and $\| \bLams \|_\infty \leq \rhons$.
%
If $\rho_n \geq \rhons$, $n \geq \sqrt{e\,n_0^3}$ and the  estimator $\hbLam$ is obtained as a solution of optimization problem \fr{opt_problem_balanced},
% with the penalty \fr{specific_pen} where  $J$ is given by \fr{eq:J_union} and $\Pen(|J|,m)$  is defined in \fr{specific_pen}.
then, for an absolute constant $\tilde{C}_0$ and any $t>0$, with probability at least $1 - 9 e^{-t}$, one has
\be \label{oracle_prob_sparse1} 
\frac{\|\hbLam - \bLams\|^2}{n^2\,L} \leq  \tilde{C}_0\, 
 \min_{J} \lfi   \frac{\| {\bPi}_{\bC^*,J}^{\bot} \btes \|^2}{n^2\,L}   +  
\frac{r_n(\ms)   \, [\Pen(|J|,\ms)   + t]}{n^2\,L} \rfi
\ee
where ${\bPi}_{\bC^*,J}$ is the projection matrix on the column space of $(\bCs \bWs^T)_J$ and $\tilde{C}_0$
is an absolute constant that depends on  $B_0$, $\aleph_1$ and $\aleph_2$ only.
 
 
In particular, if condition \fr{bias_coef_cond} holds with $K_0$ replaced with $\rhons K_0$, then
 \begin{align}  \nonumber 
\frac{\|\hbLam - \bLams\|^2}{n^2\,L} & \leq \tilde{K}_0  r_n(\ms) \, \lkr  
\min \lfi \frac{1}{L}\, \lkv \lkr \frac{\ms}{n}\rkr^2 \, \log \lkr \frac{n}{\ms} \rkr \rkv^{\frac{2\nu_0}{2 \nu_0 +1}},
\lkr \frac{\ms}{n}\rkr^2 \rfi \right. \\
& + \left.  \frac{\log \ms}{nL} + \frac{n_0}{n^2} \log \lkr \frac{\ms\, n e}{n_0} \rkr + \frac{t}{n^2 L} \rkr
\label{oracle_prob_sparse2}
 \end{align}
Here, $\tilde{K}_0$ is an absolute constant that depends on $B_0$, $K_0$, $\nu_0$,  $\aleph_1$ and $\aleph_2$ only. 
Results similar to \fr{oracle_prob_sparse1}  and \fr{oracle_prob_sparse2} hold for the expectations.
\end{theorem} 
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Dynamic graphon estimation}
\label{sec:dyn_graphon}
% \setcounter{equation}{0}


Consider the situation where   tensor  $\bLam$ is generated by a dynamic graphon $f$,
so that $\bLam$ is given by expression \fr{graphon}
% \be \label{graphon}
% \bLam_{i,j,l} = f(\zeta_i, \zeta_j, t_l), \quad i,j = 1, \cdots,n, \ l=1, \cdots, L,
% \ee
where function $f: [0,1]^3 \to [0,1]$ is such that  
$f(x,y,t) = f(y,x,t)$  for any $t$  and 
$\bzeta = (\zeta_1, \cdots, \zeta_n)$ is a random vector sampled from a distribution $\PP_\zeta$
supported on $[0,1]^n$. 


Given an observed adjacency tensor $\bB$ sampled according to model \fr{graphon}, 
the graphon function $f$ is not identifiable since the topology of  a network 
is invariant with respect to any change of labeling of its nodes. Therefore, 
for any $f$ and any measure-preserving bijection $\mu: [0,1]\to [0,1]$
(with respect to Lebesgue measure), the functions $f(x,y,t)$ and $f(\mu(x), \mu(y),t)$
define the same probability distribution on random graphs. For this reason, we are 
considering equivalence classes of graphons. Note that in order for it to be possible to compare 
clustering of nodes across time instants, we introduce an assumption 
% (which, to the best of our knowledge, first appeared in Matias and Miele (2015)) 
that there are no label switching in time, that is, every  
node carries the same label at any time $t_l$, so that function $\mu$ is independent of $t$.



Under this condition, we further assume that probabilities $\bLam_{i,j,l}$ do not change drastically 
from one time point to another, i.e. that, for every $x$ and $y$,
functions $f(x,y,t)$ are smooth in $t$. We shall also assume that $f$ is piecewise smooth
in $x$ and $y$. 
%
In order to quantify those assumptions, for each $x, y \in [0,1]^2$,  we consider a vector 
$\bof(x,y) = (f(x,y,t_1), \cdots,f(x,y,t_L))^T$ and   an orthogonal transform $\bH$ used in the previous sections.
We assume that elements $\bv_l(x,y)$ of vector $\bv(x,y) = \bH \bof(x,y)$ satisfy the following assumption:
 


{\bf (A2). } There exist constants $0 = \beta_0 < \beta_1 < \cdots < \beta_r =1$ and  $\nu_1, \nu_2, K_{1}, K_2>0$  
such that for any $x,x' \in (\beta_{i-1}, \beta_i]$ and  $y,y' \in (\beta_{j-1}, \beta_j]$, $1 \leq i,j \leq r$,
one has
\beqn    
 [\bv_l(x,y)- \bv_l(x',y')]^2   & \leq &   K_1  [|x - x'| + |y - y'|]^{2 \nu_1}, \label{A1}\\ 
\sum_{l=1}^L (l-1)^{2 \nu_2}\, \bv_l^2(x,y)  & \leq & K_2. \label{A2}
\eeqn 
%
Note that,  for a graphon corresponding to the  DSBM model,
on each of the rectangles $(\beta_{i-1}, \beta_i] \times (\beta_{j-1}, \beta_j]$,
functions $\bv_l(x,y)$ are constant, so that $\bv_l(x,y) = 0$ for $l=2, \cdots, L$, and $\nu_1 = \infty$. 
% 
\\

% \noindent 
% Observe that Condition \fr{A2} is somewhat similar to \fr{bias_coef_cond} in Assumption {\bf (A1)}
% since both assumptions 
 
 

We denote the class of graphons satisfying assumptions \fr{graphon}, \fr{A1} and \fr{A2}   by $\Sig(\nu_1, \nu_2, K_1, K_2)$.
In order to estimate  the dynamic graphon, we approximate it by an appropriate DSBM   
and then estimate the probability tensor of the DSBM.   
%
Note that, since $\nu_1, \nu_2, K_{1}$ and $K_2$
in Assumption {\bf A} are independent of $x$ and $y$, one can simplify the optimization procedure in \fr{opt_problem}.  


Let $\bQ$ be the matrix defined in \fr{con-memb} and \fr{full_model}. 
Note that since random variables $\zeta_1, \ldots, \zeta_n$ are time-independent,
we can approximate the graphon by a DSBM where group memberships of the nodes 
do not change in time. Hence, , matrices $\bC^{(l)}$ are independent of $l$, 
so that $\bC^{(l)}= \bZ$,   \fr{con-memb} holds and $\bTe = \bZ \bQ$. 
Denote $\bX = \bA \bH^T$. 
%
Denote by $\bV$ and $\bPhi$ the matrices of the coefficients of $\bQ$ and $\bTe$ in the transform $\bH$:
$\bV = \bQ \bH^T$ and $\bPhi = \bTe \bH^T$. 
% Note that $\bV$ and $\bPhi$ represent the coefficients of, respectively, $\bQ$ and $\bTe$ in the transform $\bH$.  
Then, by \fr{con-memb}, $\bTe =   \bZ \bV \bH$ and $\bPhi =  \bZ \bV$.
Note that each row of the matrices $\bV$ and $\bPhi$ corresponds to one spatial location.
Since, due to \fr{A2}, the coefficients in the transform $\bH$ decrease uniformly irrespective of the location,
one can employ   $L_1 <L$ columns instead of $L$ columns  in the final representations of $\bQ$ and $\bTe$.
In order to simplify our presentation, we denote $L_1 = L^\rho$ where $0 < \rho \leq 1$
and use the optimization procedure \fr{opt_problem} to find $m,\rho, \bV^{(\rho)}$ and $\bZ$
where $\bV^{(\rho)}$ is the submatrix of $\bV$ with columns $\bV_{*,j}$, $1 \leq j \leq L^\rho$.
%
Due to $|J| = M L^\rho$ and $0.5\, m^2 L^\rho \leq |J|  \leq m^2 L^\rho$,  
 in this case   optimization problem \fr{opt_problem} can be reformulated as
\begin{align} \label{graphon_opt}
(\hm, \hrho,\hbV^{(\hrho)}, \hbZ) & = \underset{\stackrel{m,\rho,\bV^{(\rho)}}{\bZ\in \calZ (m,n,0,L)}}
{\operatorname{argmin}} 
\lkv \|\bX^{(\rho)} - \bZ \bV^{(\rho)} \|^2 \right. \\
& \left.+ 11 n \log m + \frac{11}{2}  m^2 L^\rho \log(25  L^{1-\rho})\rkv \nonumber
% \quad \mbox{s.t.}\   \bZ \in \calZ (m,n,0,L)
\end{align}
where $\calZ (m,n,0,L)$ is defined in \fr{card_clust}. 
%
Then the estimation algorithm appears as follows:

 

\begin{enumerate}
%
\item
Apply  transform $\bH$ to the data matrix $\bA$ obtaining matrix $\bX = \bA \bH^T$.
\item
Consider a set $\Re = \lfi \rho \in (0,1]:\ L^{\rho} \quad \mbox{is an integer} \rfi$.
For every $\rho \in \Re$, remove all columns $\bX_{*,l}$ with $l \geq L^\rho+1$  obtaining matrix $\bX^{(\rho)}$ with 
$\EE \bX^{(\rho)} = \bZ \bV^{(\rho)} \equiv \bPhi^{(\rho)}$
where matrix $\bV^{(\rho)}$ has $L^\rho$ columns. 
\item
Find $(\hm, \hrho,\hbV^{(\hrho)}, \hbZ)$ as a solution of the optimization problem \fr{graphon_opt}.
\item 
Choose $\hbTe = \hbZ \hbV^{(\hrho)} \bH$ and obtain $\hbLam$ by packing $\hbTe$ into a tensor. 
\end{enumerate}

\noindent
Note  that construction of the estimator $\hbLam$ does not require  knowledge 
of  $\nu_1, \nu_2, K_{1}$ and $K_2$, so the estimator is fully adaptive.
%
The following statement provides a  minimax  upper bound for the risk of   $\hbLam$.



\begin{theorem} \label{th:upper_graphon}
Let $\Sig \equiv \Sig(\nu_1, \nu_2, K_1, K_2)$ be the class of   graphons 
satisfying Assumptions \fr{graphon}, \fr{A1} and \fr{A2}. 
If $\hbLam$ is   obtained as a solution of optimization problem  \fr{graphon_opt} as described above, then
%
\begin{align} \label{graph_upper}
 \sup_{f \in \Sig}  \frac{\EE \|\hbLam - \bLams\|^2}{n^2\,L} & \leq 
C \min_{\stackrel{1 \leq h \leq n-r}{0 \leq \rho \leq 1}} \lfi
\frac{L^{\rho-1}}{h^{2 \nu_1}} + \frac{I(\rho <1)}{L^{2 \rho \nu_2+1}} \right. \\
& + \left.
\frac{(h+r)^2 (1 + (1 - \rho) \log L) }{n^2 L^{1-\rho}}  
+ \frac{\log(h+r)}{n  L}  \rfi, \nonumber
\end{align}
where  the constant $C$ in \fr{graph_upper} depends on $\nu_1, \nu_2, K_{1}$ and $K_2$ only. 
\end{theorem}



Note that $h$ in \fr{graph_upper} stands for $h = m-r$ where $m$ is the number  of
blocks in the DSBM which approximates the graphon, hence, $h \leq n-r$.  
On the other hand, $h\geq 0$ since one needs at least $r$ blocks to approximate the graphon 
that satisfies condition \fr{A1}. Since the  expression in the right hand side 
of \fr{graph_upper} is  rather complex and is hard to analyze,
we shall consider only two regimes: a)\, $r = r_{n,L} \geq 2$ may depend on $n$ and $L$   
and $\nu_1 = \infty$; or b)\, $r = r_0\geq 1$ is a fixed  quantity independent of $n$ and $L$.
The first regime corresponds to a piecewise constant (in $x$ and $y$) graphon that generates the DSBM
while the second   regime deals with the situation where $f$ is a piecewise smooth function 
of all three arguments with a finite number of jumps.
In the first case, we set $h=2$, in the second case, we choose $h$ to be a function of $n$ and $L$. 
By minimizing the right-hand side of \fr{graph_upper}, we obtain the following statement.

 


\begin{corollary} \label{cor:upper_graphon}
Let $\hbLam$ be   obtained as a solution of optimization problem  \fr{graphon_opt} as described above. 
Then, for $\Sig \equiv \Sig(\nu_1, \nu_2, K_1, K_2)$ and $C$ independent of $n$ and $L$, one has 
{\small \be \label{graph_upper_cases}
 \sup_{f \in \Sig} \,  \frac{\EE \|\hbLam - \bLams\|^2}{n^2\,L} \leq 
\lfi \begin{array}{l}
C \min \lfi \frac{1}{L} \lkv \lkr \frac{r}{n}\rkr^2 \log \lkr \frac{n}{r}\rkr \rkv^{\frac{2 \nu_2}{2 \nu_2+1}};   
\lkr \frac{r}{n}\rkr^2 \rfi + \frac{C  \log r}{nL}, \  r = r_{n,L};\\
%
  \\
%
C \min \lfi \frac{1}{L}   \lkr \frac{\log L}{n^2}\rkr^{\frac{2 \nu_1 \nu_2}{(\nu_1 +1) (2\nu_2+1)}};   
\lkr \frac{\log L}{n^2}\rkr^{\frac{\nu_1}{\nu_1 +1}} \rfi + \frac{C  \log n}{nL},  \  r = r_0.
\end{array} \right.
\ee}
\end{corollary}


\noindent
In order to assess optimality of the penalized least squares estimator obtained above, we 
derive lower bounds for the minimax risk over the set $\Sig(\nu_1, \nu_2, K_1, K_2)$.
These lower bounds are constructed separately for each of the two regimes.  

 
 

\begin{theorem} \label{th:lower_graphon}
Let matrix $\bH$ satisfy assumptions \fr{H_assump} and $\nu_2 \geq 1/2$ in \fr{A2}. Then, 
for   $C$ independent of $n$ and $L$, one has 
\be \label{graph_lower_ineq}
\inf_{\hbLam}\ \sup_{f \in \Sig(\nu_1, \nu_2, K_1, K_2)} \  
\PP_{\bLam} \lfi \frac{\|\hbLam - \bLam \|^2}{n^2\,L} \geq   \Del(n,L)  \rfi \geq \frac{1}{4},
\ee
where
\be \label{graph_lower_cases}
\Del(n,L) = \lfi \begin{array}{l}
 C\, \min \lfi \frac{1}{L} \lkv \lkr \frac{r}{n}\rkr^2  \rkv^{\frac{2 \nu_2}{2 \nu_2+1}}; \ 
\lkr \frac{r}{n}\rkr^2 \rfi + \frac{C\, \log r}{nL}, \  r = r_{n,L};\\
%
  \\
%
C\,   \min \lfi \frac{1}{L}   \lkr \frac{1}{n^2}\rkr^{\frac{2 \nu_1 \nu_2}{(\nu_1 +1) (2\nu_2+1)}}; \ 
\lkr \frac{1}{n^2}\rkr^{\frac{\nu_1}{\nu_1 +1}} \rfi + \frac{C\, \log n}{nL}, \  r = r_0.
\end{array} \right.
\ee
%
\end{theorem}
 


 \noindent
It is easy to see that   the value of $\Del(n,L)$ coincides with the upper bound in 
\fr{graph_upper_cases} up to a  at most a  logarithmic factor of $n/r$ or $L$.
In both cases, the  first quantities in the minimums correspond  to the situation where $f$ is smooth enough as a function of time, so that 
application of transform $\bH$ improves estimation precision by reducing the number of parameters 
that needs to be estimated. The second quantities represent  the case where one needs to keep all elements of  vector $\bd$
and hence application of the transform yields no benefits. The latter can be due to the fact that  $\nu_2$ is too small or $L$ is too low.



The upper and the lower bounds in Theorems \ref{th:upper_graphon} and \ref{th:lower_graphon}
look somewhat similar to the ones appearing in  anisotropic functions estimation (see, e.g., Lepski  (2015)).
Note also that although in the case of a time-independent   graphon ($L=1$), the estimation precision does not improve 
if $\nu_1>1$, this is not true any more in the case of a dynamic graphon. Indeed,  the right-hand sides in 
\fr{graph_lower_cases} become significantly smaller when   $\nu_1, \nu_2$ or $L$ grow. 

 
\begin{remark} \label{rem:DynGraphon}
{\bf (The DSBM and the dynamic graphon).}
{\rm 
Observe that   the definition \fr{graphon} of the dynamic graphon 
assumes that vector $\bzeta$ is independent of $t$.
This is due to the fact that, to the best of our knowledge, the notion of the dynamic graphon
with $\bzeta$ being a function of time has not yet been developed by the probability community. 
For this reason,  we   restrict  our attention to the case where we are certain that, at any time point, the graphon 
describes the limiting behavior of the network  as $n \to \infty$.
Nevertheless, we believe that when the concept of the dynamic graphon is established, 
our techniques will be useful for its  estimation.


In the case of a piecewise constant graphon,  our setting corresponds to the 
situation where the nodes of the network do not switch their group memberships in time, so that $n_0 =0$
in \fr{card_clust}. Therefore,   a piecewise constant graphon ($r = r_{n,L},\ \nu_1 = \infty$) 
is just a particular case of the general DSBM since the latter allows any temporal changes of nodes' memberships. 
However, the dynamic piecewise constant graphon formulation enables us to derive specific minimax convergence 
rates for estimators of $\bLam$ in terms of $n$, $L$ and $r$.
On the other hand, the piecewise smooth graphon ($r = r_0,\ \nu_1 < \infty$) is an entirely different object 
that is not represented by the DSBM.
}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Discussion} 
\label{sec:discussion}
% \setcounter{equation}{0}


In the present paper we considered estimation of connection probabilities 
in the context of   dynamic network models.  To the best of our knowledge, 
this is the first paper to propose a fully non-parametric  model for the 
time-dependent networks which treats connection probabilities for each group as the
functional data  and allows to exploit the consistency in the group memberships over time.
The paper derives adaptive penalized least squares estimators of the tensor of the connection probabilities 
in a non-asymptotic setting  and shows that the estimators are indeed minimax optimal by
constructing the lower bounds for the risk. This is done via vectorization technique which 
is very useful for the task in the paper and can be very beneficial for solution of other problems such as, 
e.g., inference in   bi-clustering models mentioned in Remark~\ref{rem:biclust}.
In addition, we show that the correct penalty consists of two parts: the portion which accounts 
for the complexity of estimation  and the portion which accounts for the complexity of clustering and is 
proportional to the logarithm of the cardinality 
of the set of clustering matrices. The latter is a novel result and it is obtained by using the 
innovative Packing lemma   (Lemma~4) which can be viewed as a version 
% innovative Packing lemma   (Lemma~\ref{lem:packing}) which can be viewed as a version 
of the Varshamov-Gilbert  lemma for clustering matrices.   
%
Finally,  the methodologies of the paper  allow a variety of extensions.

\begin{enumerate}

\item
{\bf (Inhomogeneous or non-smooth connection probabilities). } 
Assumption \fr{A2} essentially implies that probabilities of connections are spatially homogeneous 
 and are represented by smooth  functions of time that belong to the same Sobolev class.
The model, however, can be easily generalized. First, by letting  $\bH$ be a wavelet transform and
assuming that for any fixed $x$ and $y$, function $f(x,y,\cdot)$  belongs  to a Besov ball, one can accommodate the case where
$f(x,y,\cdot)$ has   jump discontinuities. Furthermore, by using a weaker version of 
condition \fr{A2}, similarly to how this was done in Klopp and Pensky (2015), we can 
treat the case where   functions $f(x,y,t)$ are spatially inhomogeneous.

  

\item
 {\bf (Time-dependent  number of nodes). }
One can apply the theory above even when the number of nodes in the network changes from one time instant to another. 
Indeed, in this case we can form a set   that includes all nodes that have ever been in the network and 
denote their number by $n$. Consider a class $\Om_0$ such that all nodes in this class 
have zero probability of interaction with each other or any other node in the network. 
At each time instant, place all nodes that are not in the network into the class $\Om_0$. After that, 
one just needs to modify the optimization procedures  by placing additional restriction  that 
the out-of-the-network nodes indeed belong to class $\Om_0$ and that
$\bG_{0,k,l}=0$ for any $k=0,1,2,\cdots, m$ and $l=1, \cdots, L$.
 

\item
{\bf (Adaptivity to clustering complexity). }
Although, in the case of the DSBM, our estimator is adaptive
to the unknown number of classes, it requires  knowledge about 
the complexity of the set of clustering matrices.
% how much nodes' memberships can change from one time instant to another.
For example, if at most $n_0$ nodes can change their memberships between two consecutive 
time points  and  $n_0$ is a fixed quantity independent of $n$ and $m$, we 
can replace $n_0$ by $\log n$ that dominates $n_0$ if $n$ is large enough. 
However, if $n_0$ depends on $n$ and $m$,
 development of an adaptive estimator would require an additional investigation.
 

\end{enumerate}

\vspace{4mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent
{\bf SUPPLEMENTARY MATERIAL. }  Supplement contains proofs of all statements in the paper



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 

\begin{thebibliography}{10}


\bibitem{amini}
Amini, A.A., Levina, E. (2018)
On semidefinite relaxations for the block model.
{\it Ann. Statist.}, {\bf     46}, 149--179.

 

\bibitem{leonardi} 
Anagnostopoulos, A., Lacki, J., Lattanzi, S., Leonardi, S., Mahdian, M. (2016)  
Community Detection on Evolving Graphs. NIPS, 35223530.



\bibitem{bickel1}
Bickel, P.J., Chen, A. (2009) A nonparametric view of network models and newmangirvan and
other modularities. {\it Proc. Nation. Acad.  Sciences}, {\bf 106(50)}, 21068--21073.

\bibitem{bickel2}
Bickel, P.J.,  Chen, A., Levina, E. (2011) The method of moments and degree distributions
for network models. {\it Ann. Statist.}, {\bf 39}, 2280--2301.

\bibitem{borgs}
Borgs, C., Chayes, J.T., Cohn, H., Ganguly, S. (2016)
Consistent nonparametric estimation of heavy-tailed 
sparse graphs. {\tt ArXiv:1508.06675v2}


\bibitem{durante1}
Durante, D.,  Dunson, D.B.,  Vogelstein, J.T. (2015)
Nonparametric Bayes Modeling of Populations of Networks.
{\it Journ. Amer. Statist. Assoc.}, accepted. 

 

\bibitem{durante2}
Durante, D., Yang, X.  Dunson, D.B.  (2016)
Locally adaptive dynamic networks.
{\it  Ann. Appl. Stat.}, {\bf 10},   2203--2232.


 
\bibitem{gao} 
Gao, C.,   Lu, Yu,    Zhou, H.H. (2015)
Rate-optimal graphon estimation. 
{\it Ann. Statist.}, {\bf 43}, 2624--2652.


\bibitem{gao0}
Gao, C., Ma, Z., Zhang, S.Y., Zhou, H.H. (2017)
Achieving Optimal Misclassification Proportion
in Stochastic Block Models. 
{\it Journ. Machine Learn. Research}, {\bf 18}, 1--45.
 
 
 
\bibitem{gao1} 
Gao, C.,   Lu, Yu,  Ma, Z.,   Zhou, H.H. (2016) 
Optimal Estimation and Completion of Matrices with
Biclustering Structures.  
{\it Journ. Machine Learn. Research}, {\bf 17}, 1--29.


\bibitem{goldenberg}
Goldenberg, A.,  Zheng, A.X.,   Fienberg, S.E.,   Airoldi, E. M. (2010). 
A survey of statistical network models. 
{\it Found. Trends Mach. Learn.}, {\bf 2}, 129--233.


\bibitem{Gupta} 
Gupta, A.K., Nagar, D.K. (2000)
{\it Matrix Variate Distributions.}  Chapman\& Hall/CRC Press.


\bibitem{han_xu}
Han, Q.,  Xu, K.S.,  Airoldi, E.M. (2015)
Consistent estimation of dynamic and multi-layer block models. 
% {\tt ArXiv:1410.8597}
{\it Proceedings of the 32nd International Conference on Machine
Learning}, Lille, France, 2015,  1511--1520.


\bibitem{hsu}
Hsu, D.,     Kakade, S.M.,   Zhang, T. (2012)
A tail inequality for quadratic forms
of subgaussian random vectors.
{\it Electron. Commun. Probab.}, {\bf 17(52)},   1--6.


\bibitem{kloppen}
Klopp, O., Pensky, M. (2015)
Sparse high-dimensional varying coefficient model: 
non-asymptotic minimax study. {\it Ann. Stat.},  {\bf 43}, 
1273--1299.


\bibitem{klopp}
Klopp, O., Tsybakov, A.B., Verzelen, N. (2017)
Oracle inequalities for network models and sparse graphon
estimation.    {\it Ann. Statist.}, {\bf     45},   316--354.

 

\bibitem{kolaczyk}
Kolaczyk, E. D. (2009). 
{\it Statistical Analysis of Network Data: Methods and Models.} Springer.


\bibitem{kolar}
Kolar, M., Song, L., Ahmed, A., Xing, E.P. (2010)
Estimating time-varying networks. 
{\it Ann. Appl. Statist.}, {\bf 4}, 94--123.


 
\bibitem{lee}
Lee, M.,  Shen, H.,  Huang,  J. Z.,  Marron, J.S.(2010) 
Biclustering via sparse singular value decomposition. 
{\it Biometrics}, {\bf 66},  1087--1095.


 

\bibitem{lepski}
Lepski, O.  (2015)
Adaptive estimation over anisotropic functional classes via oracle approach.
{\it   Ann. Statist.}, {\bf 43},  1178--1242.

  
\bibitem{lovaszsz}
Lov\'{a}sz, L., Szegedy, B. (2006) 
Limits of dense graph sequences.  
{\it Journ. Combinatorial Theory, Ser. B}, {\bf 96}, 933--957.
 
\bibitem{lovasz}
Lov\'{a}sz, L. (2012)
{\it Large Networks and Graph Limits}.  
Colloquium Publications of the  American Mathematical Society, {\bf 60}, 
American Mathematical Society, Providence, RI.


\bibitem{massart}
Massart, P. (2007) {\it Concentration Inequalities and Model Selection.}
Springer-Verlag, Berlin, Heidelberg.


\bibitem{matias}
Matias, C.,  Miele, V. (2017) 
Statistical clustering of temporal networks through a dynamic
stochastic block model.  {\it Journ. Royal Stat. Soc., Ser. B}, {\bf 79},
1119--1141. 
 


\bibitem{minhas}
Minhas, S., Hoff, P.D., Warda, M.D. (2015)
Relax, Tensors Are Here.
Dependencies in International Processes.
{\tt ArXiv:1504.08218}


\bibitem{olhede}
Olhede, S.C., Wolfe, P. J.  (2014) 
Network histograms and universality of blockmodel approximation.
{\it  Proceed. Nat. Acad. Sci.},  {\bf 111},   14722--14727.



\bibitem{pollard}
Pollard, D. (1990)
{\it Empitrical Processes: Theory and Applications.}
Institute of Mathematical Statistics.
 
\bibitem{tsybakov}
Tsybakov, A. B. (2009). 
{\it Introduction to Nonparametric Estimation.} Springer, New
York.


\bibitem{vershynin}
 Vershynin, R.   (2012). 
Introduction to the non-asymptotic analysis of random matrices.  
In {\it  Compressed Sensing, Theory and Applications}, ed. Y. Eldar and G. Kutyniok, Chapter 5.
 {Cambridge University Press}. 


\bibitem{wolfe}
Wolfe, P.J.,  Olhede, S.C.(2013) Nonparametric graphon estimation.  
{\tt ArXiv:1309.5936}.


\bibitem{xing}
Xing, E.P., Fu, W.,  Song, L. (2010)
A state-space mixed membership blockmodel for dynamic network tomography.
{\it Ann. Appl. Stat.}, {\bf 4}, 535--566.
    
 

\bibitem{xu}
Xu, K.S. (2015)
Stochastic Block Transition Models for Dynamic Networks.
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)
2015, San Diego, CA, USA. JMLR: W\&CP, {\bf 38}.


\bibitem{xu_hero}
Xu, K.S., Hero III, A.O. (2014)
Dynamic stochastic blockmodels for
time-evolving social networks.
{\it IEEE Journal of Selected Topics in Signal Processing},  {\bf 8(4)},   
552--562.

\bibitem{yang}
Yang, T.,  Chi, Y., Zhu, S., Gong, Y.,  Jin, R. (2011)
Detecting communities and their evolutions in dynamic
social networksa Bayesian approach.
{\it Mach. Learn.}, {\bf  82}, 157--189.

 
\bibitem{zhang}
Zhang, A.Y.,  Zhou, H.H. (2016)
Minimax rates of community detection in stochastic block models.
{\it   Ann. Statist.}, {\bf 44},  2252--2280.



% \bibitem{levina}
% Zhang, Y.,  Levina, E.,    Zhu, J. (2015)
% Estimating network edge probabilities by neighborhood
% smoothing. {\tt ArXiv:1509.08588}.


 
\end{thebibliography}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\pagestyle{plain}
\setcounter{page}{1}
\pagenumbering{arabic}

 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\input{DSBM_AOS_Suppl_March25_2018.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55

 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 

  


 

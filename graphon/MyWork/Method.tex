%!TEX root = Main.tex


\section{Method} \label{sec:method}

\subsection{Evaluating the distance function}
In practice, the distance  between a point process $N$ and a probability distribution function $f$ is 
evaluated by computing distance between their (smoothed) probability distribution functions, because the distance between cumulative distribution functions can be misleading in some certain cases. 
We illustrate this through figure \ref{pic: pdf better}. Although the black dash pdf is closer to the green pdf than the red pdf, it is closer to the red curve in terms of cdf.


\begin{figure}[H]
\includegraphics[width=0.49\textwidth]{../simulation/plots/pdf_vs_cdf_c}
\includegraphics[width=0.49\textwidth]{../simulation/plots/pdf_vs_cdf_d}
\caption{ Distance between cdfs is misleading. Top left: pdf to be shifted (black dash line) and target pdf (green and red solid line). Top right: corresponding cdf. Bottom left: distance between pdfs as a function of shift. Bottom right: distance between cdfs as a function of shift.}
\label{pic: pdf better}
\end{figure}


However, as shown in figure \ref{pic: cdf better}, aligning cdf can help us to locate the minimizer of distance function, especially when their is a flat area between two pdfs.

\begin{figure}[H]
\includegraphics[width=0.49\textwidth]{../simulation/plots/pdf_vs_cdf_a}
\includegraphics[width=0.49\textwidth]{../simulation/plots/pdf_vs_cdf_b}
\caption{ cdf can help to locate the minimizer. Top left: pdf to be shifted (dash line) and target pdf (solid line). Top right: corresponding cdfs. Bottom left: distance between pdfs as a function of shift. Bottom right: distance between cdfs as a function of shift.}
\label{pic: cdf better}
\end{figure}

Based on these properties of cdf and pdf, we align pdfs using gradient descent algorithm with initialization obtained by aligning their corresponding cdfs. 
We use the shape-invariant method proposed by \citet{Bigot2013}.
The idea is to find the optimal shift that aligns $f$ and $g$ best in the Fourier domain.
Let $\theta_j$ and $\gamma_j, j=-(N-1)/2,\cdots,(N-1)/2$ be the discrete Fourier coefficients of $f$ and $g$, where $N$ is the length of discretized $f$ and $g$. The time shift parameter $\tau$ can be estimated by solving the following problem by gradient descent

	\begin{equation}
	\begin{aligned}\label{eq:time shift}
	\hat n&=
	\underset{|n|\leq (N-1)/2}{\arg\min}
	\sum_{|j|\leq (N-1)/2}
	\left| \theta_{j}e^{\complexunit 2\pi j n/N}
	-\gamma_{j} \right|^2
	,
	\end{aligned}
	\end{equation}
where $n=N\cdot\tau/2T$.
Initialization is given by the alignment result of their cdfs.
For details see \ref{app}.

\noindent
[add details for obtaining smooth pdf.]




\subsection{Main steps}
We iterate between three main steps: re-center step, re-cluster step, and re-align step.
In the re-center step, we update connecting patterns based on current clustering  and time lags; 
in the re-cluster step, we update the clusters based on updated connecting patterns and current time lags;
in the re-align step, we update time lags based on updated clusters.

\subsubsection*{Re-center step}
% In this step, we estimate connecting patterns  between each pair of clusters $\left\{ p_{q,l} \right\}_{q,l\in[k]}$.
Based on current estimation of clusters $\{  \Gamma_q^{(c)} \}_{q\in[k]}$ and time shifts $\{\tau^{(c)}_{i,j}\}_{i,j\in[n]}$, 
we can obtain shifted event times $t^{(c)}_{i,j} = \tilde t_{i,j}-\tau^{(c)}_{i,j}$.
Let $N_{q,l}$ be a point process with events at time $T_{q,l}\equiv\{t_{i,j}^{(c)}: t_{i,j}^{(c)}<\infty, {i\in\Gamma_q^{(c)},j\in\Gamma_l^{(c)} }\}$,
the connecting patterns are then estimated by kernel density estimation
\begin{align}
p_{q,l}^{(u)}(t) &= \frac{1}{h |T_{q,l}|}\sum_{t'\in T_{q,l}} K(\frac{t-t'}{h})
\label{eq: update connecting patterns},
\end{align}
where $K$ is a kernel function and $h$ is the bandwidth.



\subsubsection*{Re-cluster step}
% In this step we use the updated connecting patterns $\{p_{q,l}^{(u)}\}_{q,l\in[k]}$ to update clusters $\{\Gamma_q\}_{q\in[k]}$ and $\left\{  \tau_i \right\}_{i\in [n]}$.
Given $\{p_{q,l}^{(u)}\}_{q,l\in[k]}$ and $\{\tau^{(c)}_{i,j}\}_{i,j\in[n]}$,
we need to solve 
\begin{align*}
z_i^{(u)} 
=
\arg\min_{q\in[k]} \sum_{l} w_{i,l} \cdot \|f_{i,l} - p^{(u)}_{q,l} \|_2^2, \qquad i=1,\cdots,n .
\end{align*}
Recall that $f_{i,l} $ is the density of $\{t_{i,j}^{(c)}=\tilde t_{i,j}-\tau_{i,j}^{(c)} : t_{i,j}^{(c)}<\infty, {j\in\Gamma_l\}}$ depending on the cluster $\Gamma_l$ that is to be estimated, so the estimation of $z_i^{(u)}$ is not straightforward. 
To handle this problem, we propose to evaluate $f_{i,l} $ by computing the density of $\{t_{i,j}^{(c)}: t_{i,j}^{(c)}<\infty, {j\in\Gamma_l^{(c)}\}}$.
The optimization problem then becomes
\begin{align*}
z_i^{(u)} 
=
\arg\min_{q\in[k]} \sum_{l} w_{i,l} \cdot \|f_{i,l^{(c)}} - p^{(u)}_{q,l} \|_2^2, \qquad i=1,\cdots,n .
\end{align*}
Furthermore, to rule out the effect of time lag between $f_{i,l^{(c)}}$ and $p^{(u)}_{q,l} $ and only consider the distance between shapes, 
we substitute the $\ell_2$-distance by its shift-invariant version $d(f, g) = \inf_\tau \|f(\cdot+\tau)-g\|_2 $ and write down the final optimization problem
\begin{align}
z_i^{(u)} 
=
\arg\min_{q\in[k]} \sum_{l} w_{i,l} \cdot d(f_{i,l^{(c)}}, p^{(u)}_{q,l})^2, \qquad i=1,\cdots,n .
\label{eq: update clusters}
\end{align}
The evaluation of this shift-invariant distance will be discussed later.
Computing and comparing all the desired distances will solve this problem.




\subsubsection*{Re-align step}
Recall the structure of $\tau$ in \eqref{eq: structure of tau}. 
Based on current clusters, we will estimate $\mathbf{v }^{(u)}$ and $\mathbf{L}^{(u)}$ which determines $\tau^{(u)}$.

For each cluster $\Gamma_q^{(u)}$, randomly select a node $i^*\in\Gamma_q^{(u)}$ as a point of reference. 
By aligning node $i\in\Gamma_q^{(u)}$ and $i^*$ with respect to each cluster,
their time lag $v_i^{(u)}-v_{i^*}^{(u)}$ has $k$ possible values
\begin{align*}
	\Delta v_{(i,i^*),l} 
	&=  \arg\min_{\tau} 
	\| \tilde f_{i,l^{(u)}}(\cdot+\tau) 
	- \tilde f_{i^*,l^{(u)}}(\cdot)\|_2 , \quad l\in[k],
\end{align*}
where $\tilde f_{i,l^{(u)}}$ is the density of $\{\tilde t_{i,j}: \tilde t_{i,j}<\infty, j\in\Gamma_l^{(u)} \}$.
We will use the one with the largest magnitude as the estimated time lag:
\begin{align}
v_i^{(u)}-v_{i^*}^{(u)} 
&= \Delta v_{(i,i^*),l_0} ,  \text{ where }  l_0 = \arg\max_{l\in[k]} |\Delta v_{(i,i^*),l}|.
\label{eq: align i and i^*}
\end{align}
The reason behind this is the following. 
If $L_{q,l}=0$, the connection between node $i/i^*$ and cluster $\Gamma_l$ is led by nodes in $\Gamma_l$, and $\tilde t_{i,j}=t_{i,j}+v_i$, $\tilde t_{i^*,j}=t_{i^*,j}+v_{i^*}\overset{d}{=}t_{i,j}+v_{i^*}$ for $j\in\Gamma_l$, 
as a result $\Delta v_{(i,i^*),l}$ will be close to $v_i - v_{i^*}$; 
% while if $L_{q,l}=1$, 
% $\tilde t_{i,j}=t_{i,j}+v_j$ and $\tilde t_{i^*,j}=t_{i^*,j}+v_{j}\overset{d}{=}t_{i,j}+v_{j}=\tilde t_{i,j}$ for $j\in\Gamma_l$, 
otherwise $\Delta v_{(i,i^*),l}$ will be close to zero.
Thus non-zero values are considered as valid estimation of time lags.
\\
Setting $\min\{v_i^{(u)}\}_{i\in\Gamma_q^{(u)}}$ as zero yields estimated $\{v_i^{(u)}\}_{i\in\Gamma_q^{(u)}}$. 

$\mathbf{L}$ is then updated by comparing within group variances.
For cluster pair $(q,l)$, 
let 
\begin{align*}
\text{var}_1 &= \text{var}\left[\{ \tilde t_{i,j}-v_i^{(u)} \}_{i\in\Gamma_q^{(u)}, j\in\Gamma_l^{(u)}}\right],
\\
\text{var}_2 &= \text{var}\left[\{ \tilde t_{i,j}-v_j^{(u)} \}_{i\in\Gamma_q^{(u)}, j\in\Gamma_l^{(u)}}\right],
\end{align*}
then
\begin{align}
L_{q,l}^{(u)} = \mathbf{1}\{\text{var}_1 < \text{var}_2\}.
\end{align}


Finally, the time shifts are updated as
\begin{align}
\tau_{i,j}^{(u)} = v_i^{(u)} \mathbf{1} \{L_{q,l}^{(u)}=0\} + v_j^{(u)} \mathbf{1} \{L_{q,l}^{(u)}=1\}
.
\label{eq: update time shifts}
\end{align}


% [Add how to deal with negative event time after alignment.]









\subsection{Initialization}

Estimate $\{v_i^{(0)}\}_{i\in[n]}$ based on $\Gamma_1^{(0)}=[n]$.
Initialize time lags by $\tau_{i,j}^{(c)}=v_i^{(0)}$.
Clustering 
$\{  \Gamma_q^{(c)} \}_{q\in[k]}$ is then initialized by applying k-means++ algorithm to 
\begin{align*}
\begin{bmatrix}
f_{1}\\
f_2\\
\vdots\\
f_n
\end{bmatrix},
\end{align*}
where $f_i$ is the density of $\{\tilde t_{i,j} - v_i^{(0)}: \tilde t_{i,j}<\infty, j\in[n]\}$.




\subsection{Summary of algorithm}

The algorithm is summarized as following. 

\begin{algorithm}[H]
\SetAlgoLined
 Initialize $\{  \Gamma_q^{(c)} \}_{q\in[k]}$ and $\{  \tau_{i,j}^{(c)} \}_{i,j\in [n]}$\;
\While{$\{\Gamma_q^{(c)}\}_{q\in[k]} \neq \{\Gamma_q^{(u)}\}_{q\in[k]}$}{
  	Update $\{p_{q,l}^{(u)}\}_{q,l\in[k]}$ via \eqref{eq: update connecting patterns} with $\{\Gamma_q^{(c)}\}_{q\in[k]}$ and $\{\tau_{i,j}^{(c)}\}_{i\in[n]}$ 
  	\;
  	Update $\{\Gamma_q^{(u)}\}_{q\in[k]}$ via \eqref{eq: update clusters} with $\{p_{q,l}^{(u)}\}_{q,l\in[k]}$, $\{\Gamma_q^{(c)}\}_{q\in[k]}$ and $\{\tau_{i,j}^{(c)}\}_{i\in[n]}$ 
  	\;
  	Update $\{\tau_{i,j}^{(u)}\}_{i\in[n]}$ via \eqref{eq: update time shifts} with $\{\Gamma_q^{(u)}\}_{q\in[k]}$
  	\;
  	Evaluate the stopping criterion
  	\;
  	$\{p_{q,l}^{(c)}\}_{q,l\in[k]} \leftarrow \{p_{q,l}^{(u)}\}_{q,l\in[k]} $
  	\;
  	$\{\Gamma_q^{(c)}\}_{q\in[k]} \leftarrow \{\Gamma_q^{(u)}\}_{q\in[k]} $
  	\;
  	$\{\tau_{i,j}^{(c)}\}_{i\in[n]} \leftarrow \{\tau_{i,j}^{(u)}\}_{i\in[n]} $
  	\;
 }

\KwOut{$\{p_{q,l}^{(c)}\}_{q,l\in[k]}$, $\{\Gamma_q^{(c)}\}_{q\in[k]}$, $\{\tau_{i,j}^{(c)}\}_{i\in[n]} $.}
 \caption{ [name of algorithm]}
\end{algorithm}





		
		% \subsubsection*{k-means in point processes}
		% 	By Assumption \ref{asp:time lag} and \ref{asp:same distr}, 
		% 	% $\lambda_{N_i}(t)=\lambda_{z_i}(t-\tau_i)+\sum_{l=1}^kf_{z_i,l}(t-\tau_i)\epsilon_{i,l}$, hence 
		% 	$\lambda_{N_i}(t+\tau_i)\overset{d}{=}\lambda_{N_j}(t+\tau_j)$ for any $i,j$ satisfying $z_i=z_j$. Thus $F_{N_i}(t+\tau_i)\overset{d}{=}F_{N_j}(t+\tau_j)$ for any $i,j$ satisfying $z_i=z_j$, 
		% 	where 
		% 	$F_{N_i}(t):=\int_0^t\lambda_{N_i}(s)\text{d}s\big/\int_0^T\lambda_{N_i}(s)\text{d}s$.
		% 	So the k-means problem is to solve
		% 	% \begin{equation}\label{eq:kmeans_lambda}
		% 	% \min_{\left\{ \Gamma_l \right\}_{l=1}^k} \frac{1}{n} \sum_{l=1}^k \left(\min_{\{\tau_i\}_{i\in\Gamma_l},\lambda_l} \sum_{i\in\Gamma_l} \|\hat\lambda_{N_i}(\cdot+\tau_i)- \lambda_l(\cdot)\|_2^2 \right),
		% 	% \end{equation}
		% 	% or, equivalently,
		% 	\begin{align}\label{eq:kmeans_F}
		% 	\min_{\left\{ \Gamma_l \right\}_{l=1}^k} \frac{1}{n} \sum_{l=1}^k \left(\min_{\{\tau_i\}_{i\in\Gamma_l},F_l} \sum_{i\in\Gamma_l} \| \tilde{F}_{N_i}(\cdot+\tau_i)- F_l(\cdot)\|_2^2 \right),
		% 	\end{align}
		% 	where 
		% 	% $\hat \lambda_{N_i}(\cdot)$ is the intensity function estimated from $N_i(\cdot)$ using some smooth method,
		% 	$\tilde{F}_{N_i}(t):=1/{N_i([0,T])}\cdot\sum_{j=1}^{N_i([0,T])}\mathbf{1}_{\{t_{N_i,j}\leq t\}}$ is the empirical distribution function of the occurrence time of edges,
		% 	$t_{N_i,j}$ is the occurrence time of the $j$-th edge of $N_i(\cdot)$,
		% 	and $F_l(t)=\mathbb{E}[F_{N_i}(t+\tau_i)] (\forall i\in\Gamma_l)$ is the expected cumulative distribution function of the $l$-th cluster.


		% 	% Likelihood can also be used as a measure of similarity between a point process and an intensify function,
		% 	% which yields the objective function
		% 	% \begin{align}\label{eq:kmeans_likelihood}
		% 	% \min_{\left\{ \Gamma_l \right\}_{l=1}^k} \frac{1}{n} \sum_{l=1}^k \left(\min_{\{\tau_i\}_{i\in\Gamma_l},\lambda_l} \sum_{i\in\Gamma_l} d\big({N_i}(\cdot+\tau_i), \lambda_l(\cdot)\big) \right),
		% 	% \end{align}
		% 	% where the distance is defined as the negative log-likelihood
		% 	% \begin{align*}
		% 	% d(N, \lambda) := -\log L(N;\lambda) = \int_{0}^T\lambda(t)\text{d}t - \sum_{j=1}^{N(0,T]}\log \left( \lambda(t_{N,j}) \right) .
		% 	% \end{align*}
				
		% 	% {\color{blue} Justify why it is reasonable to use Poisson process. }
		% 	% % Also explain why this is a good measure of similarity.
	 % 		% {\color{blue} See \citet{Daley} for details.}

	 % 	% \subsubsection*{Other possible distance}
		% 	% The squared error distance is defined as 
		% 	% \begin{align*}
		% 	% d(N,\lambda) := \int_0^T\lambda^2(t)\text{d} t - 2\int_0^T \lambda(t)\text{d}N(t)+?
		% 	% \end{align*}



	% \subsection{Algorithm}
		
	% 	The initialization method and the choice of the number of clusters $k$ will be discussed later, for now we assume the initialization and $k$ are given.
	% 	To solve the problem \eqref{eq:kmeans_F}, we iterate between two steps until convergence:
	% 		\begin{itemize}
	% 			\item Re-cluster step: update the clustering 
	% 			$\left\{ \hat\Gamma_l \right\}_{l=1}^k$ based on the distance $d(\tilde{F}_{N_i}, \hat F_l)$ 
	% 			defined as 
	% 			\begin{align*}
	% 			 d(\tilde{F}_{N_i}, \hat F_l) = \min\Big\{ \inf_{\tau\in[0,T]}\left( \int_{-T}^T\big| S_\tau\circ\tilde{F}^*_{N_i}(t)-\hat F_{l}^*(t) \big|^2 \text{d}t \right)^{1/2}, \\
	% 			 \inf_{\tau\in[0,T]}\left( \int_{-T}^T\big| \tilde{F}^*_{N_i}(t)-S_\tau\circ\hat F_{l}^*(t) \big|^2 \text{d}t \right)^{1/2} \Big\},
	% 			\end{align*}
	% 			where
	% 			\begin{align}\label{eq:def of shifted curve}
	% 			S_\tau\circ\tilde{F}^*_{N_i}(t)=
	% 			\begin{cases}
	% 			0, &t\in[-T,-\tau)\\
	% 			\tilde F_{N_i}(t+ \tau), &t\in[-\tau,T-\tau)\\
	% 			1,& t\in [T-\tau,T]
	% 			\end{cases},
	% 			% \quad
	% 			\hat F_{l}^*(t)=
	% 			\begin{cases}
	% 			0, &t\in[-T,0)\\
	% 			\hat F_{l}(t), &t\in [0,T]
	% 			\end{cases}.
	% 			\end{align}
				
				
	% 			\item Re-center step: update the expected cumulative distribution functions $\{\hat F_l\}_{l=1}^k$ using the method in \citet{Bigot2013}.

	% 		\end{itemize}

	% 	In the re-cluster step, the distance $d(\tilde F_{N_i}, \hat F_l)$ for each pair of node $i$ and cluster $l$ is evaluated by solving the problem
	% 	\begin{equation}
	% 	\begin{aligned}\label{eq:aligned distance}
	% 	\hat n_{i,l}&=
	% 	\underset{|n|\leq (N-1)/2}{\arg\min}
	% 	\sum_{0<|j|\leq (N-1)/2}
	% 	\left| \theta_{i,j}e^{\complexunit 2\pi j n/N}+
	% 	\frac{e^{\complexunit{}2\pi j n/N}-1}{1-e^{\complexunit{}2\pi j /N}}
	% 	-\gamma_{l,j} \right|^2
	% 	+ \left| \theta_0+n-\gamma_0 \right|^2
	% 	\\
	% 	&=\underset{|n|\leq (N-1)/2}{\arg\min}
	% 	\sum_{0<|j|\leq (N-1)/2}
	% 	\left| \left( \theta_{i,j}+ \frac{1}{1-e^{\complexunit{}2\pi j /N}} \right) e^{\complexunit{}2\pi j n/N} 
	% 	- \left( \gamma_{l,j}+\frac{1}{1-e^{\complexunit{}2\pi j /N}} \right)  \right|^2 
	% 	+\\
	% 	&\hspace{11cm} \left| \theta_0+n-\gamma_0 \right|^2
	% 	\\
	% 	&\overset{\triangle}{=}
	% 	\underset{|n|\leq (N-1)/2}{\arg\min}\sum_{0<|j|\leq (N-1)/2}\left| \theta_{i,j}' e^{\complexunit{}2\pi j n/N} - {\gamma_{l,j}'}  \right|^2
	% 	+ \left| \theta_0+n-\gamma_0 \right|^2
	% 	,
	% 	\end{aligned}
	% 	\end{equation}
	% 	where $n=N\cdot\tau/2T$,
	% 	$\theta_{i,j}$ and $\gamma_{l,j}$,
	% 	$j=  -(N-1)/2,\cdots,(N-1)/2 $, are the discrete Fourier coefficients of $\tilde F^*_{N_i}$ and $\hat F^*_l$,
	% 	$N$ is the length of discretized version of $\tilde F^*_{N_i}$ and $\hat F^*_l$.
	% 	% Grid search can be used to solve problem \eqref{eq:aligned distance}.
	% 	\\
	% 	Gradient descent can be used to solve the problem. 
	% 	The gradient is shown below:
	% 	% Gradient:
	% 	\begin{equation}
	% 	\begin{split}
	% 	\nabla_{n_i} = \frac{4\pi}{N}\cdot 
	% 	\sum_{0<|j|\leq (N-1)/2} j\cdot 
	% 	\text{Im} \left( {\theta_{i,j}'}\overline{{\gamma_{i,j}'}}e^{\complexunit2\pi n_i j/N} \right) 
	% 	+ 2n_0+2 \text{Re}\left( \theta_0-\gamma_0 \right) .
	% 	\end{split}
	% 	\end{equation}
	% 	The learning rate was set as $0.01$, the initialization of $n_i$ was set to be the last estimated $\hat n_i$.
		


	% 	In the re-center step, let $\left\{ \theta_{i,j} \right\}_{j\in \mathbb{Z}}$ and $S_\tau\circ\tilde{F}^*_{N_i}(t)$ be defined the same as above.
	% 	The Fourier coefficients of $S_\tau\circ\tilde{F}^*_{N_i}(t)$ are
	% 	\begin{align*}
	% 	\theta_{i,j}e^{\complexunit 2\pi j n/N}+
	% 	\frac{1}{1-e^{\complexunit{}2\pi j /N}}
	% 	\left( e^{\complexunit{}2\pi j n/N}-1 \right) 
	% 	\overset{\triangle}{=}
	% 	\theta_{i,j}' e^{\complexunit{}2\pi j n/N}-C. 
	% 	\end{align*}
	% 	Then
	% 	$\left\{ \hat \tau_i \right\}_{i=1}^n = \left\{ \hat n_i\cdot 2T/N \right\}_{i=1}^n$ can be obtained by 
	% 	\begin{align*}
	% 	\left\{ \hat n_i \right\}_{i\in\Gamma_l} &= 
	% 	\underset{\min_{i\in\Gamma_l} \left\{ n_i \right\}=0}{\arg\min}
	% 	\frac{1}{|\Gamma_l|}
	% 	\sum_{i\in\Gamma_l}
	% 	\sum_{j=1}^N 
	% 	\left| 
	% 	\theta_{i,j}' e^{\complexunit{}2\pi j n_i/N} -
	% 	\frac{1}{|\Gamma_l|}\sum_{i'\in\Gamma_l}\theta_{i',j}' e^{\complexunit{}2\pi j n_{i'}/N}  
	% 	\right|^2 
	% 	.
	% 	\end{align*}
	% 	Using gradient descent over all $\left\{ n_i \right\}_{i\in\Gamma_l}$ is expensive, thus we adopt the following two-step estimation procedure:
	% 	\begin{itemize}
	% 		\item Estimate the mean distribution function $\hat F^*_l(t)$.
	% 		\item Estimate $\left\{ \hat n_i \right\}_{i\in\Gamma_l}$ by aligning each $\tilde F^*_{N_i}(t) $ with $\hat F^*_l(t)$.
	% 	\end{itemize}
	% 	The initialization of $\hat F^*_l(t)$ can be any randomly chosen $\tilde F^*_{N_i}(t) $.

	% 	% gradient:
	% 	% \begin{align*}
	% 	%  \frac{\partial L}{\partial \tau_i} = 
	% 	%  \frac{1}{|\Gamma_l|^2}
	% 	%  \sum_{|j|\leq m} 4\pi j
	% 	%  \left( 
	% 	%  	\text{Im}\left( \theta_{i,j}' e^{\complexunit{}2\pi j \tau_i}\cdot\overline{A_j}\right)+
	% 	%  	\left(\frac{1}{|\Gamma_l|}-2\right) \text{Im}(A_j)\cdot 2 \text{Re}\left( \theta_{i,j}' e^{\complexunit{}2\pi j \tau_i}   \right) 
	% 	%  \right) ,
	% 	%  \end{align*}
	% 	%  where $A_j=\frac{1}{|\Gamma_l|}\sum_{i\in\Gamma_l}\theta_{i,j}'e^{\complexunit{}2\pi j \tau_i} $.

	% 	Finally, the mean distribution function is estimated as the average of shifted empirical distribution functions.		  



		% {\color{red} Should we take into account the error in $t_{N_i,j}$?}\\
		


		
		% Another direction is to use negative log-likelihood as the distance.
		% For reference see \cite{Vimond2010,Gamboa2007,Ronn2009,Gervini2005}.
		% The maximum likelihood estimator of the mean curve proposed in \citet{Gervini2005} is showed to be $\sqrt{n}$-consistent and asymptotically normal. % {(converge to a Gaussian process)}
		% {\color {red} But the log-likelihood includes unknown $f_{l,l'}$}
		% \begin{align*}
		% L(\lambda_{l},\tau_i;N_i)=\int \mathbb{P}\left( N_i(t)\Big| \lambda_{l}(t-\tau_i)+\sum_{l'=1}^k f_{l,l'}(t-\tau_i)\epsilon_{i,l'} \right) f(\epsilon_{i,1})\cdots f(\epsilon_{i,k})\\\text{d}\epsilon_{i,1}\cdots \text{d}\epsilon_{i,k}
		% \end{align*}
			


	% \subsection{Convex relaxation of k-means type clustering}
	% 	\subsubsection*{Semidefinite programming relaxation}
	% 		We briefly introduce a semidefinite programming relaxation (Peng-Wei relaxation) of k-means proposed by \citet{Peng2005}.
	% 		The k-means objective function in \eqref{eq:kmeans} can be re-written as
	% 		\begin{align*}
	% 		\sum_{l=1}^k\sum_{i\in\Gamma_l}\|\mathbf{x}_i- \mathbf{c}_l\|^2 &= \frac{1}{2}\sum_{l=1}^k \frac{1}{|\Gamma_l|}\sum_{i,j\in\Gamma_l}\|\mathbf{x}_i - \mathbf{x}_j\|^2\\
	% 		&= \frac{1}{2}\sum_{l=1}^k\frac{1}{|\Gamma_l|}\langle \mathbf{1}_{\Gamma_l}\mathbf{1}_{\Gamma_l}^\top,\mathbf{D} \rangle
	% 		\end{align*}
	% 		where $\mathbf{D}\in \mathbb{R}^{n\times n}$ with entries $\mathbf{D}_{ij}=\|\mathbf{x}_i- \mathbf{x}_j\|^2$.
	% 		\\
	% 		Hence \eqref{eq:kmeans} can be relaxed to
	% 		\begin{align*}
	% 		&\min_{\mathbf{Z}}\ \langle \mathbf{Z},\mathbf{D}\rangle \qquad \\
	% 		& \ \text{s.t. } \quad \mathbf{Z} \succeq 0, \quad \mathbf{Z} \geq 0, \quad \mathbf{Z} 1_{n}=\mathbf{1}_{n}, \quad \operatorname{Tr}(\mathbf{Z})=k.
	% 		\end{align*}
	% 	Proximity conditions are discussed in \ref{sec:proximity condition}.
		
	% 	In our case, \eqref{eq:kmeans_lambda} is equivalent to 
	% 		\begin{equation}\label{eq:unconvexified k-means}
	% 		\min_{\mathbf{Z},\left\{ \tau_i \right\}_{i=1}^n}\langle \mathbf{Z}, \mathbf D(\tau_1,\cdots,\tau_n)  \rangle
	% 		\end{equation}
	% 	where $\mathbf{Z}=1/2\cdot \sum_{l=1}^k(1/|\Gamma_l|\cdot \mathbf{1}_{\Gamma_l}\mathbf{1}_{\Gamma_l}^\top)$, $\mathbf{D}_{i,j} = \| \hat\lambda_{N_i}(t+\tau_i)-\hat\lambda_{N_j}(t+\tau_j) \|_2^2$.
	% 	Using Fourier transformation, 
	% 	\begin{align*}
	% 	\mathbf{D}_{i,j} &= \int_{0}^T \left( \int_0^\infty \hat h_{N_i}(\xi)e^{i2\pi\xi(t+\tau_i)}-\hat h_{N_j}(\xi)e^{i2\pi\xi(t+\tau_j)}\text{d}\xi \right)^2 \text{d}t
	% 	\end{align*}
	% 	where $\hat h_{N_i}(\xi)$ is the Fourier transform of $\hat\lambda_{N_i}(t)$. 
	% 	% This seems not a convex function of $\tau_i$'s.
	% 	{\color{red} How to convexify?}
		
		
		

	% \subsection{Other thoughts}
	% 	\begin{itemize}
	% 		\item What about using Fourier transformation and then k-means? What about functional principal component analysis?
	% 	\end{itemize}







